---

layout: post
tags: [LLM, NLP，MOE]
title: MOE模型小窥
date: 2025-2-18
author: Kyrie Chen
comments: true
toc: true
pinned: false

---

Mixture of Experts (MoE)架构是一种先进的机器学习模型设计，特别适用于大语言模型（LLM）。在MoE架构中，整个模型被划分为多个专门的子网络（称为“专家Experts”），每个专家针对特定类型的数据或任务进行训练。通过一个门控网络，MoE能够动态选择和激活与输入数据最相关的专家，从而实现稀疏计算。这种方法使得在处理复杂任务时，模型能够显著减少计算成本，同时提高性能和效率。

# Introduction

在大语言模型中，MoE架构的优势尤为明显。它允许模型在保持较大参数规模的同时，仅激活部分专家进行计算，从而降低了预训练和推理阶段的计算需求。例如，在某些实现中，MoE可以在一次前向传播中仅激活少数几个专家，这样可以在不牺牲性能的情况下，显著提高计算效率和响应速度。这使得MoE成为处理自然语言处理等领域中多样化数据和高计算需求任务的重要工具。

## 现有MOE模型汇总

| 模型 | 发布    | 规模   | 训练   | 备注       |
|:------|:-------------|:----------|:--------|:------------|
|[Deepseek](https://huggingface.co/deepseek-ai/DeepSeek-V2.5)|DeepSeek于2024年12月10日发布并持续更新| 16B (激活2.4B)、236B （激活22B）| 未披露|当前最优秀的MoE系列大模型之一|
|[Qwen2.5-54B-A14B](https://huggingface.co/Qwen/Qwen2-57B-A14B)|Alibaba于2024年5月发布|54B (激活14B) |包含大规模文本与代码数据 (包含中文和英文数据)，采用MOE与密集层结合| 针对聊天生成任务进行了优化，适合多种应用场景。(HF地址里写的是57B, 需要核实是否为笔误)|
|[Mixtral](https://arxiv.org/abs/2401.04088)|Mistral AI于2024年1月发布|46.7B (8*7B)|采用稀疏MoE架构|8个专家，每次选择2个。|
|[Arctic](https://www.snowflake.com/en/blog/arctic-open-efficient-foundation-language-models-snowflake/)|Snowflake于2024年4月发布|480B (128x3.66B 激活17B)|动态数据课程，包含代码、文本等数据，强调数据质量和多样性。包含密集MoE混合变换器架构，使用1000张GPU，训练时间约3周|开源模型，性能指标与其他LLM相当。|
|[DBRX](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)|Databricks于2024年3月发布|132B (36B活跃参数)|12T文本和代码数据，强调代码数据质量和多样性，包含多种编程语言。包含精细化MoE架构。训练使用3072张H100，约3个月时间，包括了pretraining, post-training, evaluation, red-teaming, and refining|16个专家，每次选择4个。在多个基准测试中表现优越，尤其在编程任务上超越GPT-3.5。|
|[Grok-1]()|xAI于2023年11月发布|推测数十亿参数量 (具体未公开)|可能包含大量社交媒体数据 (特别是X平台的数据)。推测使用了Transformer架构，具体细节未公开。训练推测使用了大量高性能GPU。|xAI公司产品，与X（原推特）平台深度整合。|
|[Grok-2]()|xAI于2024年8月发布|推测较Grok-1有所增加|包括大规模文本与图像数据，改进的MoE架构，使用约10万张NVIDIA H100 GPU进行训练|是Grok-1的升级版本，推理、编码和多模态理解能力显著提升。|
|[Grok-3]()|xAI于2025年2月18日发布|未知|包括大规模文本与图像数据，改进的MoE架构，使用约10万张NVIDIA H100 GPU进行训练|在自然语言理解和生成方面有显著提升，具体细节尚未披露。|
|[OLMoE](https://arxiv.org/pdf/2409.02060)|Allenai于2024年9月24日发布|7B(1B激活参数)|包含大规模英文语料，尝试了对现有MoE算法的改进，并第一次将这个规模的MoE模型在5T tokens的语料上进行训练。使用 256 个 H100 GPU，通过 GPU 间的 NVlink 连接和节点间的InfiniBand 互连，大约进行 10 天的训练。|同等规模下最好的MoE模型|
