---

layout: post
tags: [LLM, NLP，MOE]
title: MOE模型小窥
date: 2025-2-18
author: Kyrie Chen
comments: true
toc: true
pinned: false

---

Mixture of Experts (MoE)架构是一种先进的机器学习模型设计，特别适用于大语言模型（LLM）。在MoE架构中，整个模型被划分为多个专门的子网络（称为“专家Experts”），每个专家针对特定类型的数据或任务进行训练。通过一个门控网络，MoE能够动态选择和激活与输入数据最相关的专家，从而实现稀疏计算。这种方法使得在处理复杂任务时，模型能够显著减少计算成本，同时提高性能和效率。

# Introduction

在大语言模型中，MoE架构的优势尤为明显。它允许模型在保持较大参数规模的同时，仅激活部分专家进行计算，从而降低了预训练和推理阶段的计算需求。例如，在某些实现中，MoE可以在一次前向传播中仅激活少数几个专家，这样可以在不牺牲性能的情况下，显著提高计算效率和响应速度。这使得MoE成为处理自然语言处理等领域中多样化数据和高计算需求任务的重要工具。

## 现有MOE模型汇总

| 模型 | 发布    | 规模   | 训练   | 备注       |
|:------|:-------------|:----------|:--------|:------------|
|[Deepseek](https://huggingface.co/deepseek-ai/DeepSeek-V2.5)|DeepSeek于2024年12月10日发布并持续更新| 16B (激活2.4B)、236B （激活22B）| 未披露|当前最优秀的MoE系列大模型之一|
|[Qwen2.5-54B-A14B](https://huggingface.co/Qwen/Qwen2-57B-A14B)|Alibaba于2024年5月发布|54B (激活14B) |包含大规模文本与代码数据 (包含中文和英文数据)，采用MOE与密集层结合| 针对聊天生成任务进行了优化，适合多种应用场景。(HF地址里写的是57B, 需要核实是否为笔误)|
|[Mixtral](https://arxiv.org/abs/2401.04088)|Mistral AI于2024年1月发布|46.7B (8*7B)|采用稀疏MoE架构|8个专家，每次选择2个。|
|[Arctic](https://www.snowflake.com/en/blog/arctic-open-efficient-foundation-language-models-snowflake/)|Snowflake于2024年4月发布|480B (128x3.66B 激活17B)|动态数据课程，包含代码、文本等数据，强调数据质量和多样性。包含密集MoE混合变换器架构，使用1000张GPU，训练时间约3周|开源模型，性能指标与其他LLM相当。|
|[DBRX](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)|Databricks于2024年3月发布|132B (36B活跃参数)|12T文本和代码数据，强调代码数据质量和多样性，包含多种编程语言。包含精细化MoE架构。训练使用3072张H100，约3个月时间，包括了pretraining, post-training, evaluation, red-teaming, and refining|16个专家，每次选择4个。在多个基准测试中表现优越，尤其在编程任务上超越GPT-3.5。|
|[Grok-1]()|xAI于2023年11月发布|推测数十亿参数量 (具体未公开)|可能包含大量社交媒体数据 (特别是X平台的数据)。推测使用了Transformer架构，具体细节未公开。训练推测使用了大量高性能GPU。|xAI公司产品，与X（原推特）平台深度整合。|
|[Grok-2]()|xAI于2024年8月发布|推测较Grok-1有所增加|包括大规模文本与图像数据，改进的MoE架构，使用约10万张NVIDIA H100 GPU进行训练|是Grok-1的升级版本，推理、编码和多模态理解能力显著提升。|
|[Grok-3]()|xAI于2025年2月18日发布|未知|包括大规模文本与图像数据，改进的MoE架构，使用约10万张NVIDIA H100 GPU进行训练|在自然语言理解和生成方面有显著提升，具体细节尚未披露。|
|[OLMoE](https://arxiv.org/pdf/2409.02060)|Allenai于2024年9月24日发布|7B(1B激活参数)|包含大规模英文语料，尝试了对现有MoE算法的改进，并第一次将这个规模的MoE模型在5T tokens的语料上进行训练。使用 256 个 H100 GPU，通过 GPU 间的 NVlink 连接和节点间的InfiniBand 互连，大约进行 10 天的训练。|同等规模下最好的MoE模型|

# Architecture

## MOE的基本组成
  - 专家（Experts）
    混合专家模型 (MoE) 的理念起源于 1991 年的论文 Adaptive Mixture of Local Experts。这个概念与集成学习方法相似，旨在为由多个单独网络组成的系统建立一个监管机制。在这种系统中，每个网络 (被称为“专家”) 处理训练样本的不同子集，专注于输入空间的特定区域。那么，如何选择哪个专家来处理特定的输入呢？这就是门控网络发挥作用的地方，它决定了分配给每个专家的权重。在训练过程中，这些专家和门控网络都同时接受训练，以优化它们的性能和决策能力。
    
    在 2010 至 2015 年间，两个独立的研究领域为混合专家模型 (MoE) 的后续发展做出了显著贡献:

    1. 组件专家: 在传统的 MoE 设置中，整个系统由一个门控网络和多个专家组成。在支持向量机 (SVMs) 、高斯过程和其他方法的研究中，MoE 通常被视为整个模型的一部分。然而，Eigen、Ranzato 和 Ilya 的研究 探索了将 MoE 作为更深层网络的一个组件。这种方法允许将 MoE 嵌入到多层网络中的某一层，使得模型既大又高效。
    2. 条件计算: 传统的神经网络通过每一层处理所有输入数据。在这一时期，Yoshua Bengio 等研究人员开始探索基于输入令牌动态激活或停用网络组件的方法。
    
    这些研究的融合促进了在自然语言处理 (NLP) 领域对混合专家模型的探索。特别是在 2017 年，Shazeer 等人 (团队包括 Geoffrey Hinton 和 Jeff Dean，后者有时被戏称为 “谷歌的 Chuck Norris”) 将这一概念应用于 137B 的 LSTM (当时被广泛应用于 NLP 的架构，由 Schmidhuber 提出)。通过引入稀疏性，这项工作在保持极高规模的同时实现了快速的推理速度。这项工作主要集中在翻译领域，但面临着如高通信成本和训练不稳定性等多种挑战。

    ![](https://raw.githubusercontent.com/kakack/kakack.github.io/master/_images/250218_01.png)

    混合专家模型 (MoE) 的引入使得训练具有数千亿甚至万亿参数的模型成为可能，如开源的 1.6 万亿参数的 Switch Transformers 等。这种技术不仅在自然语言处理 (NLP) 领域得到了广泛应用，也开始在计算机视觉领域进行探索。然而，本篇博客文章将主要聚焦于自然语言处理领域的应用和探讨。

   - 门控网络（Gating Network）
  
    这个部分用于决定哪些令牌 (token) 被发送到哪个专家。例如，在下图中，“More”这个令牌可能被发送到第二个专家，而“Parameters”这个令牌被发送到第一个专家。有时，一个令牌甚至可以被发送到多个专家。令牌的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。

    ![](https://raw.githubusercontent.com/kakack/kakack.github.io/master/_images/250218_02.png)

    总结来说，在混合专家模型 (MoE) 中，我们将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个门控网络和若干数量的专家。门控网络决定了哪些令牌被发送到哪个专家，而专家则负责处理这些令牌。这种方法的引入使得模型能够在保持极高规模的同时实现快速的推理速度。

  - 输出组合机制（Combiner）
      
    一般来说，组合器将所有专家模型的输出进行加权融合

##  MOE的工作原理

