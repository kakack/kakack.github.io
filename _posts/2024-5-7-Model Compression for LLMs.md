---

layout: post
tags: [LLM, NLP, Model Compression]
title: Model Compression for LLMs
date: 2024-05-7
author: Kaka Chen
comments: true
toc: true
pinned: false

---

# 1 - Intro

# 2 - Quatization

## 2.1 - Post-Training Quantization

### 2.1.1 - Weight-Only Quantization

### 2.1.2 - Weight-Activation Co-Quantization

## 2.2 - Quantization-Aware Training

# 3 - Parameter Pruning

## 3.1 - Structured Pruning

## 3.2 - Unstructured Pruning

# 4 - Low-Rank Approximation

# 5 - Knowledge Distillation

## 5.1 - White-Box KD

## 5.2 - Black-Box KD

# 6 - Conclusion
