---

layout: post
tags: [LLM, NLP, Attention]
title: Efficient Attention for LLMs
date: 2024-05-20
author: Kaka Chen
comments: true
toc: true
pinned: false

---

# 1 - Intro

# 2 - Sharing-based Attention

# 3 - Feature Information Reduction

# 4 - Kernelization or Low-Rank

# 5 - Fixed Pattern Strategies

# 6 - Learnable Pattern Strategies

# 7 - Hardware-Assisted Attention

# 8 - Conclusion
