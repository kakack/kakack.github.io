---

layout: post
categories: [Spark]
tags: [Big Data, Distributed System, Spark Streaming]

---

不管是Spark还是Hadoop，似乎每个工具一出来之后第一个例子往往是WordCount，就像是每个语言学习第一个程序HelloWorld一样。其作用可能仅仅是跑通整个计算流程，然后介绍一些在计算过程中所必须的结构、变量、包等等。于是我们今次也是从Spark Streaming的NetworkWordCount开始，一步步窥探究竟。

- - -
##NetworkWordCount

首个例子，Code：

```
object NetworkWordCount {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: NetworkWordCount <hostname> <port>")
      System.exit(1)
    }
    //要求输入命令格式是NetworkWordCount <hostname> <port>形式

    StreamingExamples.setStreamingLogLevels()

    //创建一个batch size为1的context，关于SC后文介绍
    val sparkConf = new SparkConf().setAppName("NetworkWordCount")
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    
    /*
     *在目标ip:port上创建一个socket stream来计算输入流的单词个数
     *
     */
    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }
}
```

运行结果：

![](https://raw.githubusercontent.com/kkkelsey/kkkelsey.github.io/master/_images/Screen%20Shot%202015-10-21%20at%2013.58.24.png)

其中需要先在另一个Term上运行`nc -lk 9999`作为strea的输入方才行。

这个例子十分简单，整体其实就是一个简单的Scala的做WC的app，但是用Spark Streaming的方式来做数据的输入流和监听工作。

作为一个Spark应用，其敲门砖就是SparkContext和SparkConf两个变量。

- **SparkContext**相当于是程序入口的main，来负责所有Spark有关的操作。其参数是SparkConf，用来描述应用的配置信息。

```
/**
 * Main entry point for Spark functionality. 
 * A SparkContext represents the connection to a Spark
 * cluster, and can be used to create RDDs, 
 * accumulators and broadcast variables on that cluster.
 *
 * @param config a Spark Config object 
 * describing the application configuration. 
 * Any settings in this config overrides
 * the default configs as well as system properties.
 */

class SparkContext(config: SparkConf) extends Logging 

```

- **SparkConf**是一组以key-value形式存在的键值对，通常需要通过`new SparkConf()`方法来创建， 可以从其他spark.*的Java系统属性集中载入参数，自设的SC会比系统默认的SC有更高的优先级，这个配置信息一旦传递给Spark之后，就不能再被修改了。

```
/*
 * @param loadDefaults whether to also load values from Java system properties
 */
class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging

//所有的可配置参数如下所示：
private[spark] def updatedConf(
    conf: SparkConf,
    master: String,
    appName: String,
    sparkHome: String = null,
    jars: Seq[String] = Nil,
    environment: Map[String, String] = Map()): SparkConf =
{
  val res = conf.clone()
  res.setMaster(master)
  res.setAppName(appName)
  if (sparkHome != null) {
    res.setSparkHome(sparkHome)
  }
  if (jars != null && !jars.isEmpty) {
    res.setJars(jars)
  }
  res.setExecutorEnv(environment.toSeq)
  res
}
```

关于这个例子整个步骤如图：

![](https://raw.githubusercontent.com/kkkelsey/kkkelsey.github.io/master/_images/Z7naE3.jpg)

而在这个例子中StreamingContext封装了SparkContext。

```
//StreamingContext 里面包装的还是一个SparkContext
class StreamingContext private[streaming] (
  sc_ : SparkContext,
  cp_ : Checkpoint,
  batchDur_ : Duration
  ) extends Logging {

  def this(conf: SparkConf, batchDuration: Duration) = {
  this(StreamingContext.createNewSparkContext(conf), null, batchDuration)
  }

private[streaming] def createNewSparkContext(
  master: String,
  appName: String,
  sparkHome: String,
  jars: Seq[String],
  environment: Map[String, String]
 ): SparkContext = {
  val conf = SparkContext.updatedConf(
   new SparkConf(), master, appName, sparkHome, jars, environment)
  createNewSparkContext(conf)
}
...

```

例子中的StreamingContext实例化后从SocketTextStream，如

`val lines = ssc.socketTextStream(args(1), args(2).toInt, StorageLevel.MEMORY_ONLY_SER)`

其中arg1，arg2分别时候之后监听的ip和port，随后的参数是定义储存级别的，可见这仅仅存在内存中。

StreamingContext.SocketTextStream方法如下，返回的是一个ReceiverInputDStreaming：

```
def socketTextStream(
  hostname: String,
  port: Int,
  storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2
): ReceiverInputDStream[String] = {
    socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel)
}
```

在初始化定义了StreamingContext之后，需要做的事情是：

1. 通过创建input DStream定义输入源；
2. 通过在DStream上应用transformation和output operations来定义streaming的计算；
3. 通过`streamingContext.start()`来开始接收和处理数据；
4. 通过`streamingContext.awaitTermination()`来等待整个处理过程停止（或者手动停止或报错停止）；
5. 整个处理过程可以手动通过`streamingContext.stop()`来中止。

此外，关于context：

1. 一旦一个context已经启动，不能有新的streaming computation被注册或者加入进去；
2. 一旦一个context已经停止，不能再次被启动；
3. 在一个JVM中，一次只能有一个StreamingContext存在；
4. 在StreamingContext中的`stop()`方法同样可以停止SparkContext，为了要只停止StreamingContext，可以在`stop()`方法中把可选的参数`stopSparkContext`为false；
5. 一个SparkContext可以被重复使用创建多个StreamingContext，只要之前一个StreamingContext在后一个创建之前就被中止。 

- **DStream**：AKA Discretized Streams，离散流，本质上是一种可离散化的stream，将stream离散化成一组RDD的list，所以对其的基本操作仍然是以RDD为主。

提供两个类别的built-in streaming源：

- 基本源：由StreamingContext API直接提供的源：file system、Socket connection、Akka actors
- 高级源：像kafka、flume、kinesis、twitter等，需要linking外部依赖。

![](https://raw.githubusercontent.com/kkkelsey/kkkelsey.github.io/master/_images/1408795343_7070.jpg)

```
abstract class DStream[T: ClassTag] (
 @transient private[streaming] var ssc: StreamingContext
  ) extends Serializable with Logging {

  @transient
  private[streaming] var generatedRDDs = new HashMap[Time, RDD[T]] ()
...
```

generatedRDDS是DStream的一根成员变量HashMap，Key是时间片，Value是RDD，即DStream从本质上来说是以时间片为标签存储的一系列RDD。

```
/*
 * Retrieve a precomputed RDD of this DStream, or computes the RDD. This is an internal
 * method that should not be called directly.
 */
private[streaming] def getOrCompute(time: Time): Option[RDD[T]] = {
// If this DStream was not initialized (i.e., zeroTime not set), then do it
// If RDD was already generated, then retrieve it from HashMap
  generatedRDDs.get(time) match {

// If an RDD was already generated and is being reused, then
// probably all RDDs in this DStream will be reused and hence should be cached
  case Some(oldRDD) => Some(oldRDD)

// if RDD was not generated, and if the time is valid
// (based on sliding time of this DStream), then generate the RDD
  case None => {
    if (isTimeValid(time)) {
      compute(time) match {
        case Some(newRDD) =>
          if (storageLevel != StorageLevel.NONE) {
            newRDD.persist(storageLevel)
            logInfo("Persisting RDD " + newRDD.id + " for time " +
              time + " to " + storageLevel + " at time " + time)
          }
          if (checkpointDuration != null &&
            (time - zeroTime).isMultipleOf(checkpointDuration)) {
            newRDD.checkpoint()
            logInfo("Marking RDD " + newRDD.id + " for time " + time +
              " for checkpointing at time " + time)
          }
          generatedRDDs.put(time, newRDD)
          Some(newRDD)
        case None =>
          None
      }
    } else {
      None
    }
  }
}
```

在WC例子中，从StreamingContext获取一根ReceiverInputDStream，封装了SocketReceiver对象，用来读取流数据，然后按时间片将离散化的储存在DStream定义的HashMap里。

```
def socketTextStream(
  hostname: String,
  port: Int,
  storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2
): ReceiverInputDStream[String] = {
socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel)
}
 def socketStream[T: ClassTag](
  hostname: String,
  port: Int,
  converter: (InputStream) => Iterator[T],
  storageLevel: StorageLevel
): ReceiverInputDStream[T] = {
new SocketInputDStream[T](this, hostname, port, converter, storageLevel)
}
```

以上便是整个WC example的过程，获取SocketTextStream。以下介绍一下StreamingContext启动。在启动过程中，直接或间接触发了JobScheduler、StreamingListenerBus、ReceiverTracker和JobGenerator。

```
def start(): Unit = synchronized {
    
    //首先生成一个eventActor，类型是Akka Actor
    if (eventActor != null) return 
    // scheduler has already been started

    logDebug("Starting JobScheduler")
    eventActor = ssc.env.actorSystem.actorOf(Props(new Actor {
      def receive = {
        case event: JobSchedulerEvent => processEvent(event)
      }//用processEvent方法来实现事件处理逻辑
    }), "JobScheduler")
    /**
     * 启动listenerBus
     */
    listenerBus.start()
    receiverTracker = new ReceiverTracker(ssc)
    //启动receiverTracker
    receiverTracker.start()
    jobGenerator.start()
    logInfo("Started JobScheduler")
}
```

关于processEvent方法不再赘述，大致就是一系列的handle方法来捕获Job的启动事件、完成事件、error report事件等。之后将事件信息封装传输到ListenerBus，让注册在监听器总线上的监听器都能针对job的状态变化做出反应。

最后补一份关于RDD的transformation的列表：

![](https://raw.githubusercontent.com/kkkelsey/kkkelsey.github.io/master/_images/Screen%20Shot%202015-10-22%20at%2010.17.13.png)

- - -

在另一个叫CustomReceiver的例子中，其他都与NetworkWordCount一样，不同的是用
`val lines = ssc.receiverStream(new CustomReceiver(args(0), args(1).toInt))`
代替了
`val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)`
在此new创建了一个receiverStream然后封装了一个CustomReceiver类。作为一个用户自定义的CustomReceiver类，必须继承Receiver，并且实现`onStart()`和`onStop()`两个方法

```
class CustomReceiver(host: String, port: Int)
  extends Receiver[String](StorageLevel.MEMORY_AND_DISK_2) with Logging {

  def onStart() {
    // Start the thread that receives data over a connection
    new Thread("Socket Receiver") {
      override def run() { receive() }
    }.start()
  }

  def onStop() {
   // There is nothing much to do as the thread calling receive()
   // is designed to stop by itself isStopped() returns false
  }

  /** Create a socket connection and receive data until receiver is stopped */
  private def receive() {
   var socket: Socket = null
   var userInput: String = null
   try {
     logInfo("Connecting to " + host + ":" + port)
     socket = new Socket(host, port)
     logInfo("Connected to " + host + ":" + port)
     val reader = new BufferedReader(new InputStreamReader(socket.getInputStream(), "UTF-8"))
     userInput = reader.readLine()
     while(!isStopped && userInput != null) {
       store(userInput)
       userInput = reader.readLine()
     }
     reader.close()
     socket.close()
     logInfo("Stopped receiving")
     restart("Trying to connect again")
   } catch {
     case e: java.net.ConnectException =>
       restart("Error connecting to " + host + ":" + port, e)
     case t: Throwable =>
       restart("Error receiving data", t)
   }
  }
}
// scalastyle:on println

```

- - -

另外还有个例子叫RecoverableNetworkWordCount，意思是可恢复的NWC，这也是Streaming相对于Storm而言比较优秀的一点。

```
object RecoverableNetworkWordCount {

def createContext(ip: String, port: Int, outputPath: String, checkpointDirectory: String)
//创建新的Context，下文详述

  def main(args: Array[String]) {
    if (args.length != 4) {
      System.err.println("You arguments were " + args.mkString("[", ", ", "]"))
      System.err.println(
        """
          |Usage: RecoverableNetworkWordCount <hostname> <port> <checkpoint-directory>
          |     <output-file>. <hostname> and <port> describe the TCP server that Spark
          |     Streaming would connect to receive data. <checkpoint-directory> directory to
          |     HDFS-compatible file system which checkpoint data <output-file> file to which the
          |     word counts will be appended
          |
          |In local mode, <master> should be 'local[n]' with n > 1
          |Both <checkpoint-directory> and <output-file> must be absolute paths
        """.stripMargin
      )
      System.exit(1)
    }
    val Array(ip, IntParam(port), checkpointDirectory, outputPath) = args
    val ssc = StreamingContext.getOrCreate(checkpointDirectory,
      () => {
        createContext(ip, port, outputPath, checkpointDirectory)
      })
    ssc.start()
    ssc.awaitTermination()
  }
}
// scalastyle:on println

```

解释一下这个example大致意思，输入参数从之前的ip port变成ip port <checkpoit> <output>，前两者的含义不变，依然是Socket数据传输过来的位置，<checkpoint>是一个兼容HDFS文件系统上的绝对路径，这个路径上，<output>会继续上一次的计算。方法`StreamingContext.getOrCreate(checkpointDir, CreateFunc)`方法可以做到，如果在checkpointDir路径下有已check了的StreamingContext，则重新载入恢复到已有的SC，如果没有则用CreateFunc创建一个新的SC，这个创建方法在这个例子中如下：

```
  def createContext(ip: String, port: Int, outputPath: String, checkpointDirectory: String)
    : StreamingContext = {

    // If you do not see this printed, that means the StreamingContext has been loaded
    // from the new checkpoint
    println("Creating new context")
    val outputFile = new File(outputPath)
    if (outputFile.exists()) outputFile.delete()
    val sparkConf = new SparkConf().setAppName("RecoverableNetworkWordCount")
    // Create the context with a 1 second batch size
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    ssc.checkpoint(checkpointDirectory)

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    val lines = ssc.socketTextStream(ip, port)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.foreachRDD((rdd: RDD[(String, Int)], time: Time) => {
      val counts = "Counts at time " + time + " " + rdd.collect().mkString("[", ", ", "]")
      println(counts)
      println("Appending to " + outputFile.getAbsolutePath)
      Files.append(counts + "\n", outputFile, Charset.defaultCharset())
    })
    ssc
  }
```

- - -

最后另一个例子是StatefulWordCount，特点是利用RDD的可重用性，保存中间计算状态，在有新的输入传进来之后，计算结果根据已有的状态再做累加。

```
object StatefulNetworkWordCount {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: StatefulNetworkWordCount <hostname> <port>")
      System.exit(1)
    }

    StreamingExamples.setStreamingLogLevels()

    val updateFunc = (values: Seq[Int], state: Option[Int]) => {
      val currentCount = values.sum

      val previousCount = state.getOrElse(0)

      Some(currentCount + previousCount)
    }

    val newUpdateFunc = (iterator: Iterator[(String, Seq[Int], Option[Int])]) => {
      iterator.flatMap(t => updateFunc(t._2, t._3).map(s => (t._1, s)))
    }

    val sparkConf = new SparkConf().setAppName("StatefulNetworkWordCount")
    // Create the context with a 1 second batch size
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    //就以当前路径做check point的路径
    ssc.checkpoint(".")

    // Initial RDD input to updateStateByKey，用一个本地Scala collection创建一个新的RDD用于做之后的词数累加工作。这边最初放进去了一个List
    val initialRDD = ssc.sparkContext.parallelize(List(("hello", 1), ("world", 1)))

    // Create a ReceiverInputDStream on target ip:port and count the
    // words in input stream of \n delimited test (eg. generated by 'nc')
    val lines = ssc.socketTextStream(args(0), args(1).toInt)
    val words = lines.flatMap(_.split(" "))
    val wordDstream = words.map(x => (x, 1))

    // Update the cumulative count using updateStateByKey
    // This will give a Dstream made of state (which is the cumulative count of the words)
    //UpdateStateByKey返回一个新state的Dstream，其中每个key都根据传入的方法，这里是newUpdateFunc做了更新
    val stateDstream = wordDstream.updateStateByKey[Int](newUpdateFunc,
      new HashPartitioner (ssc.sparkContext.defaultParallelism), true, initialRDD)
    stateDstream.print()
    ssc.start()
    ssc.awaitTermination()
  }
}
// scalastyle:on println
```

返回的结果跟最初的NetworkWordCount相比，能记录之前计算结果，每次计算结果都是之前的累加。

终端演示结果：

![](https://raw.githubusercontent.com/kkkelsey/kkkelsey.github.io/master/_images/Screen%20Shot%202015-10-22%20at%2017.16.11.png)