---

layout: post
categories: [Algorithm]
tags: [Detection, Deep Learning]

---

# Abstract

对于图像中目标检测，最朴素的需求就是输入一张目标图像，输出图像中待检测目标物的位置，用bounding box形式输出，和该物体的类别，用类别标签标示。在Yolo出现之前，业界最优秀的方法是基于region proposal的R-cnn系列方法，包括rcnn、fast-rcnn和faster-rcnn。概括而言这一类的算法可以归纳成两步走的two-stage，首先通过经验手段或者elective search、rpn等方法来生成网络觉得可能会出现目标物体位置的region proposal，然后再将rp中提取到的信息通过网络，最后用分类来获得目标物体的类别，用回归来确定目标物体的bounding box位置。这种做法相对于之前其他的方法而言，大大提高了物体定位的准确率，但是也存在一个很大的问题就是处理速度慢。人们举过一个很生动的例子，如果将rcnn系列检测器放在一辆以60km/h疾驰的汽车上做物体检测，当输入一帧画面得到结果的时候，用rcnn的车子已经开出300m远，用fast-rcnn的也已经开出34m以上。因此rcnn系列算法在一些强调响应速度的应用上，会显得非常滞后。

因此，yolo在设计之初的理念就是加快这个方法的运行速度。因此它摒弃了预先生成rp的方法，直接以整图作为模型的输入，提取整图的特征信息。然后在输出的时候不再对分类结果和定位结果分别处理，而是使用回归的方法一次性统一得到位置信息bounding box和分类信息class label。最后，yolo整体上设计的是一个端到端的单独网络，简化了训练和推理过程中的复杂度。整个流程可以概括为得到输入图像，resize成输入尺寸，经过卷积神经网络得到一系列输出，再使用一些阈值方法得到我们最终希望得到的检测结果。它不需要像原先的滑动窗口或者rpn算法一样反复在图片上遍历，而只需要看图片一次就够了，这也是yolo名字的来源，`you only look once`。

# How does Yolo v1 work


## Brief work method

首先我们想象在需要被检测的图片上，分布着一系列网格，比如yolo v1中将一张图划分为7*7的grids。

![](https://raw.githubusercontent.com/kakack/kakack.github.io/master/_images/20201216-1.jpg)

其中每一个网格都会负责预测一些bounding box和对应的置信度，这个置信度表示的是对bounding box内是否有物体的信心，框的个数是预先设定的比如v1默认的是2个，负责的原则是该物体的中心点落在这个这个网格内，比如右上角的某个网格，就会预测这辆车的bounding box，同时会得到对应bounding box的置信值。如果当前的cell里没有任何物体，我们也会得到相应的bounding box，只不过我们希望这些bounding box的置信度尽可能的低。当我们把所有bounding box都拿出来，就大致知道了在我们所输入的画面里，哪些位置可能会有目标物。但此时我们还有一个问题，我们知道哪儿有目标，但不知道目标具体是什么。因此我们下一步就需要每个网格再预测出内涵物体是什么类型的概率。于是我们就得到了这样一个类似粗糙的特征图的画面，可以大致推断哪边是狗、哪边是自行车或者汽车。需要知道的一点是，当前我们得到的概率是一个条件概率，也就是说我们假定这个网格里有东西，那这个东西是什么类型的概率。换言之，当这个网格预测我当前汽车的概率时，它并不代表我这里一定有一辆汽车，而是说假如我这个网格里有物体，那么它很有可能是汽车。于是当我们拿这个条件概率去乘以之前说到的存在物体的置信度，那么我们就能得到每一个bounding box实际包含物体类别的概率。当前我们已经得到了很多bounding box，其中有一些对于任何类别的目标而言概率都很低，于是就用一些阈值来直接过滤了，剩下的bounding box可以用nms的方法来去冗余，得到我们期望的结果。

对于每个网格来说，都会去预测以下一些内容，首先是若干个bounding box，每个bounding box中包括4个坐标值`x`、`y`、`h`、`w`和一个存在物体的置信值`c`，然后需要预测多少类目标物，就需要再加各个类别的可能性。比如对yolo v1在pascal voc数据集上，预设将每个整图切分成7*7的网格，每个网格对应2个bounding box，共计20类目标标签，那么我们的一个输出y就包括7*7*（2*5+20）=7*7*30个张量=1470个输出。所以yolo对于目标类型和其位置信息的预测输出是同时的。

## Training

Yolo既然是从整图推理出检测结果，那么在训练的时候喂入的也需要是一张整图。当我们得到了一些图和关于这些图的ground truth标注后，需要做的第一件事就是将得到的ground truth和图片中合适的网格进行绑定，也就是我们希望在推理预测时能输出这个ground truth对应结果的网格。对应的方法很简单，就是标签bounding box的中心位置所在的网格即是需要负责它的网格。然后调整这个网格对于类别的预测结果，比如这个网格里，我们把dog类设置成1，其余类设置成0。同样我们需要调整这个网格的bounding box proposal，我们看这个网格提供的预测bounding box中哪一个跟最终ground truth的bounding box重合率最高，于是就调整这个bounding box的置信度和位置，然后降低其余bounding box的置信度。我们在这张图中，还有很多网格并没有包含任何ground truth的中心点，我们同样需要将他们对应的bounding box置信度降到足够低。

## Network design

![](https://raw.githubusercontent.com/kakack/kakack.github.io/master/_images/20201216-2.jpg)

Yolo v1采用卷积网络来提取特征，然后使用全连接层来得到预测值。网络结构参考`GooLeNet`模型，包含24个卷积层和2个全连接层，如图所示。对于卷积层，主要使用1x1卷积来做channle reduction，然后紧跟3x3卷积。对于卷积层和全连接层，采用Leaky ReLU激活函数：max（x， 0.1x）。但是最后一层却采用线性激活函数。可以看到网络的最后输出为 7*7*30大小的张量。这和前面的讨论是一致的。这个张量所代表的具体含义我画在右下角的图中。对于每一个单元格，其中20个元素是类别概率值，然后2个元素是bounding box的置信度，两者相乘可以得到各自类别的置信度，最后8个元素是bounding box的x、y、w和h。可能有人会感到奇怪，对于bounding box为什么把置信度和位置信息都分开排列，而不是按照（x,y,w,h,c）这样排列，其实纯粹是为了计算方便，因为实际上这30个元素都是对应一个单元格，其排列是可以任意的。但是分离排布，可以方便地提取每一个部分。在训练之前，yolo v1先在ImageNet上进行了预训练，其预训练的分类模型采用图中前20个卷积层，然后添加一个average-pool层和全连接层。预训练之后，在预训练得到的20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。由于检测任务一般需要更高清的图片，所以将网络的输入从224x224增加到了448x448。


## Loss function
