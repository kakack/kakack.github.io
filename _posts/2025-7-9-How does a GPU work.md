---

layout: post
tags: [LLM, NLP，AI Infra]
title: How does a GPU work
date: 2025-7-9
author: Kyrie Chen
comments: true
toc: true
pinned: false

---


# **一、 引言：为什么是GPU？**

在探讨GPU如何工作之前，我们首先要回答一个更根本的问题：为什么AI的发展离不开GPU？

答案的核心在于AI，尤其是深度学习，其本质是**大规模的矩阵和向量运算**。无论是训练一个复杂的神经网络，还是运行一个大语言模型（LLM）进行推理，其底层都涉及海量的、可以被分解为独立部分的数学计算。例如，在神经网络的前向传播中，每一层的输出都是由前一层的输入与权重矩阵相乘，再加上偏置向量得到的。这个过程需要对成千上万个数字进行重复的、模式相同的乘法和加法运算。

这种计算任务的特点是：**计算量巨大，但逻辑简单且高度并行**。这正是GPU大显身手的舞台。GPU的设计初衷是为了处理图形渲染，而图形渲染本身也是一种高度并行的任务——屏幕上的每个像素点都可以被独立计算。这种为并行而生的架构，恰好与AI的算力需求不谋而合。因此，当我们谈论AI Infra时，GPU便成为了绕不开的基石。


# **二、 CPU vs. GPU：不同的设计哲学**

要理解GPU，最好的方式就是将它与我们更熟悉的CPU（中央处理器）进行对比。它们虽然都是处理器，但设计理念却截然不同，这决定了它们各自擅长的领域。

## **CPU：精于串行的“瑞士军刀”**

CPU被设计成一个通用的、能够处理各种复杂任务的“多面手”。它的核心特点是：
- **少数强大的核心**：一个典型的CPU通常只有几个到几十个核心。
- **复杂的控制逻辑**：每个核心都配备了复杂的控制单元、分支预测器和大量的缓存，使其能够快速处理各种复杂的指令和逻辑判断。
- **高时钟频率**：CPU追求单个任务的极致执行速度。

你可以把一个CPU核心想象成一位经验丰富、能力全面的**老教授**。他能处理各种疑难杂症，无论是复杂的逻辑推理还是精密的计算，都能应对自如。但他的精力有限，无法同时处理成千上万个简单问题。因此，CPU非常适合处理操作系统、应用程序等需要复杂逻辑判断和串行执行的任务。

![CPU与GPU架构示意图](https://raw.githubusercontent.com/kakack/kakack.github.io/master/_images/250709-1.png)
*(图注：CPU架构示意图，其特点是拥有强大的ALU（算术逻辑单元）和大量的缓存，但计算核心数量较少)*

#### **GPU：擅长并行的“人海战术”**

与CPU不同，GPU的设计目标非常专一：**大规模并行计算**。它的核心特点是：
- **成千上万的简单核心**：一块GPU芯片上集成了数千个甚至更多的计算核心（如NVIDIA的CUDA核心）。
- **简化的控制逻辑**：每个核心的控制逻辑和缓存都非常简单，它们被设计用来高效地执行同一条指令。
- **高内存带宽**：为了同时喂饱成千上万个核心，GPU配备了高带宽内存（HBM），确保数据能够被快速地传输。

你可以把GPU想象成一个由**成千上万名小学生**组成的计算方阵。虽然每个小学生的计算能力有限，也无法处理复杂的逻辑问题，但当你给他们下达一个简单的、统一的计算任务时（比如“所有人都计算1+1”），他们能以惊人的速度同时完成成千上万次计算。这正是深度学习所需要的。

![GPU架构示意图](https://raw.githubusercontent.com/kakack/kakack.github.io/master/_images/gpu_architecture.png)
*(图注：GPU架构示意图，其特点是拥有海量的ALU，控制单元和缓存相对简单)*

总而言之，CPU和GPU的设计差异决定了它们在AI计算中的分工：**CPU负责整体的逻辑控制和任务调度，而GPU则专注于执行那些计算密集型、高度并行的核心任务。** 理解了这种根本性的差异，我们才能更好地深入GPU的内部，探究其并行计算的奥秘。

---

# **三、 深入GPU架构：并行计算的奥秘**

理解了GPU“人海战术”的设计哲学后，我们来进一步拆解其内部结构，看看这支庞大的“计算军团”是如何被组织和调度的。

## **SIMD/SIMT：并行计算的灵魂**

GPU之所以能实现大规模并行，核心在于其计算模型。你可能听说过两个术语：**SIMD**（单指令，多数据）和**SIMT**（单指令，多线程）。

- **SIMD（Single Instruction, Multiple Data）**：这是并行计算的一种经典模型。它意味着**用一条指令同时对多个数据执行相同的操作**。想象一下，老师对一个班的学生说：“请大家把手里的数字都加上5”。在这里，“加5”就是单条指令，而每个学生手里的不同数字就是多份数据。

- **SIMT（Single Instruction, Multiple Thread）**：这是NVIDIA在CUDA架构中提出的模型，可以看作是SIMD在GPU上的升级版和更灵活的实现。它将成千上万的计算任务包装成**线程（Thread）**，然后将32个线程组成一个**线程束（Warp）**。同一个Warp中的所有线程在同一个时钟周期内执行相同的指令，但每个线程可以处理不同的数据。SIMT模型的美妙之处在于它对开发者更友好，屏蔽了底层硬件的复杂性。你只需要编写单个线程要执行的程序，GPU的硬件调度器会自动将它映射到成千上万个核心上去并行执行。

无论是SIMD还是SIMT，其本质都是用一条指令驱动海量的计算单元，这是GPU实现超高计算吞吐量的根本。

## **CUDA核心、Tensor Core与TPU：专业分工的计算单元**

GPU的“计算军团”并非由单一兵种构成，而是由不同类型的专业计算单元组成，以应对不同的任务需求。

- **CUDA核心（CUDA Core）**：这是GPU最基本的计算单元，主要负责执行**单精度浮点（FP32）**和整数运算。你可以把它看作是GPU里的“普通士兵”，负责执行大部分通用的并行计算任务。

- **Tensor Core（张量核心）**：这是NVIDIA从Volta架构开始引入的、**专为深度学习打造的“特种部队”**。Tensor Core专门用于执行大规模的矩阵乘加运算（Matrix Multiply-Accumulate, MMA），并且在硬件层面直接支持**混合精度（FP16/FP32）**和低精度（INT8/INT4）计算。在一次操作中，一个Tensor Core可以完成一个4x4的矩阵乘法，其效率远超CUDA核心。对于大模型训练和推理中无处不在的矩阵运算，Tensor Core能够带来数倍的性能提升。

- **与TPU的对比**：Google的TPU（Tensor Processing Unit）是另一个为AI而生的专用处理器。如果说Tensor Core是GPU里的“特种部队”，那TPU就是一支纯粹的“矩阵运算专业军团”，它将整个芯片的设计都聚焦于此，因此在特定任务上能效比极高。而GPU则更像是一个通用平台，既有CUDA核心处理通用并行任务，又有Tensor Core加速AI任务。

## **内存层次：HBM（高带宽内存）的重要性**

想象一下，你拥有一个成千上万人的计算军团，但如果后勤补给（数据供应）跟不上，这支军团的战斗力将大打折扣。这就是所谓的“**内存墙**”问题——计算单元的速度远超内存访问速度，导致计算单元不得不花费大量时间等待数据。

为了解决这个问题，现代高端GPU普遍采用**HBM（High Bandwidth Memory，高带宽内存）**。与传统的DDR内存不同，HBM通过以下方式实现了超高的带宽：
- **3D堆叠**：HBM将多个DRAM芯片垂直堆叠起来，并通过硅通孔（TSV）技术进行连接，极大地增加了数据传输的并行度。
- **宽位宽接口**：HBM拥有极宽的内存接口（如1024-bit或更高），远超DDR内存的64-bit。

你可以把DDR想象成一条普通的双车道公路，而HBM则是一条拥有32条车道的超级高速公路。通过HBM，GPU能够以极高的速度从显存中读取和写入数据，从而“喂饱”其内部成千上万个嗷嗷待哺的计算核心。

![HBM示意图](https://raw.githubusercontent.com/kakack/kakack.github.io/master/_images/hbm_architecture.png)
*(图注：HBM通过3D堆叠和宽位宽接口技术实现超高内存带宽)*

## **SM（Streaming Multiprocessor）：GPU的中枢神经**

如果说CUDA核心是士兵，那么**SM（Streaming Multiprocessor，流式多处理器）**就是军营里的“指挥官”。一块GPU由多个SM组成，每个SM都是一个高度独立的计算单元，它包含了：
- 一定数量的CUDA核心和Tensor Core。
- 自己的指令缓存和调度器。
- 一小块高速的共享内存（Shared Memory）和L1缓存。

SM是GPU执行任务的核心单位。当一个计算任务（在CUDA中称为Kernel）被启动时，它会被分解成一个个线程块（Thread Block），然后这些线程块被分配到不同的SM上去执行。SM内部的调度器再将线程块分解成线程束（Warp），并安排它们在CUDA核心或Tensor Core上执行。

**SM就像一个自给自足的计算工厂**，它接收任务，管理资源，调度执行，并最终产出结果。正是由成百上千个这样的“工厂”协同工作，才构成了GPU强大的并行处理能力。

