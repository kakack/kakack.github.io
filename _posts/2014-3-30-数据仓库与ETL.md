---

layout: post
categories: [Data Center]
tags: [Data Center,ETL]

---


我本科时期毕设的时候就做过一个线上数据中心的Demo，刚刚拿到需求的时候也很像我们平时理解的数据库，因为当时我的Demo主要要做的工作也就是CRUD操作，所以一直心里想笑，干嘛取个数据中心Data Center那么高大上的名字，其实数据中心或者数据仓库与数据库的区别还是比较明显的，简单说，数据仓库就像是一个码头，而数据库就是码头中的仓库，如果给某个码头航拍一张照片，我们看到的最大的建筑一定是巨大的集装箱仓库，但是光有仓库是不能让一个码头正常工作的，因为我们需要塔台来指挥货物的调配，需要吊车来运载货物，需要地勤，也需要各种各样的文职人员定期对仓库货物进行评估、分析，对后来需要存储的货物预测，再有效设计现有货物的利用途径等。所以说存储只是数据仓库的一项功能，而数据仓库概念提出的最主要目的是为在存储的基础上解决对数据的分析和使用，为企业提出决策支持。所以一般常见的数据仓库会有三部分组成：源数据、数据仓库、数据应用：

![](https://raw.githubusercontent.com/kakack/kakack.github.io/master/_images/140330.png)

数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（抽取Extra, 转化Transfer, 装载Load）的过程，ETL是数据仓库的流水线，也可以认为是数据仓库的血液，它维系着数据仓库中数据的新陈代谢，而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定。

- - -

## 数据源

常见的数据源有结构化数据、半结构化数据和非结构化数据三种，这三种数据源的区别大致如下：

- 结构化数据：格式清晰规范，如关系型数据库中的数据，可以用二维表储存，各个字段长度属性固定，最方便管理，也会带有像时间戳、签名等识别信息。
- 半结构化数据：格式也相对规范，一般都是纯文本数据，可以通过某种方式解析得到每项的数据。最常见的就是日志数据、XML、JSON等格式的数据，在直接使用之前需要解析，因为各个字段长度不等。
- 非结构化数据：指的是那些非纯文本类数据，没有标准格式，无法直接地解析出相应的值。常见的非结构化数据有富文本文档、网页、多媒体（图像、声音、视频等）。需要有特定的办法来使用，很难定义标准的方法来解析。一般不会直接将其存为二进制的文档，数据仓库之父——Inmon的建议是在数据仓库中只需要储存非结构化数据的元数据（Meta Data），或者称为解释型数据。所以这些非结构化数据一般会被存在文件系统中，然后在数据仓库中存储这些数据的信息和检索，以便快速查阅这些数据。对于大型网站而言，这一部分数据会被用作高级的数据挖掘，其统计分析意义不大。

- - -

## 数据存储

在存储过程中，源数据是通过ETL进行转换后以特定形式存入数据仓库。但是在具体是否存放所有源数据细节的问题上有些分歧，有的认为数据仓库主旨是面向分析，所以不必存放所有细节，另一种观点认为应该先建立和维护细节模型，再根据需求建立分析模型。但是个人认为数据分析的需求不是由数据本身决定的，而是在后期应用过程中迭代产生，需求的变更和重定向是非常频繁的，所以对于后一种存储办法显然更能应对未来可能产生的需求变更问题。

- - -

## 元数据管理

　　元数据（Meta Date），其实应该叫做解释性数据，即数据的数据。主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态。一般会通过元数据资料库（Metadata Repository）来统一地存储和管理元数据，其主要目的是使数据仓库的设计、部署、操作和管理能达成协同和一致。

- - -

## 关于ETL

ETL，Extraction-Transformation-Loading的缩写，中文名称为数据抽取、转换和加载。

1. 数据抽取：可以认为一次抽取就是把源数据的数据抽取到ODS（操作型数据储存）或者DW(Data Warehouse数据仓库)当中，这里的源数据可以是关系型数据库或者文本文件、日志文件等。其中抽取的策略是比较重要的一环。一般有全量抽取和增量抽取两种。全量抽取适用于总量小而又难以判断是否发生修改的内容，如关系表、维度表、配置表等，而增量抽取则用于那些数据量大，不可能每次都做全量抽取的数据。判断增量的办法比较多：
      - 有时间标识字段，标记源数据中修改时间，如createtime、updatetime
      - 根据上次抽取结束自增长id
      - 分析数据日志
      - 与之前数据hash值比较
      - 与源数据主动推增
2. 数据清洗：一般在抽取之后做，用于去除不合规定和没有意义的数据。清洗主要包括这些方面：
      - 空值处理：替换成某个特定值或者去除
      - 验证数据正确性
      - 规范格式
      - 数据转码
      - 统一标准
3. 数据转换和加载

常见的ETL工具有Informatica、Datastage、微软SSIS等。之前做处理的时候ETL都是学长写好的，所以在此做个简单小结。













