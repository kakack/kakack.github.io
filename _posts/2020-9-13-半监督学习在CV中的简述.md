---

layout: post
categories: [Computer Vision]
tags: [Semi-Supervised Learning, Computer Vision，Machine Learning]

---

# 半监督学习在计算机视觉中的简述 Brief Intro of Semi-Supervised Learning in Computer Vision

## 概述

当前的CV计算模式无论是早先的Machine Learning还是现在的Deep Learning解决方案，都是遵照着`ML/DL expertise+Computation+Data`的形式展开，其中ML/DL expertise代表着算法工程师们在特征工程、模型选择、模型设计、模型训练上的人为抉择，Computation是硬件承载这些方案的计算能力，而Data则是支撑所有解决方案进行的数据模块，包括参与训练的数据集和后续验证的数据集。但是工业界上，如果需要进行非常严谨的监督学习方法，针对Data而言，需要大量的人力用于对数据的人工标注。
而半监督学习的模式则是为弥补只有少数标注数据（labeled data）而有大量未标注数据的情况下，希望能和拥有全标注数据集的监督学习达到相同或类似学习效果的权宜之计。当根据手头仅有的标注数据，会将一些虽然没有标注但可被观测到的数据进行一些合理的推测，其中推测主要基于两个假设：

 - `聚类假设cluster assumption`：假设数据间具有相同的聚类结构，同一类数据会落入同一个聚类类别内。
 - `流形假设manifold assumption`：假设数据分布在一个流形上，相近的样本具有相似的预测结果。

半监督学习有四种主要方法：

 - `生成式方法(generative methods)`: 也称为`自训练算法（self-training）`假设标注与未标注图片来源于同一个生成模型，将未标注数据看作模型参数的缺失，可以用EM等方法进行估计。用有标签数据训练一个分类器，然后用这个分类器对无标签数据进行分类，这样就会产生`伪标签（pseudo label）`或`软标签（soft label）`，挑选你认为分类正确的无标签样本（此处应该有一个挑选准则），把选出来的无标签样本用来训练分类器。
 - `基于图的方法(graph-based methods)`：构建出一个图结构，节点对应训练样本，边对应样本关系，根据某些准则将标注信息在图上进行扩散。也称为`标签传播法（Label Propagation Algorithm）`。
 - `低密度分割法(low-density separation methods)`：强制分类边界穿过输入空间的低密度区域，如`S3VMs`。
 - `基于分歧的方法(disagreement methods)`：其实也是自训练的一种，算法会生成多个学习器，并让它们合作来挖掘未标注数据，其中不同学习器之间的分歧是让学习过程持续进行的关键，如`联合训练（co-training）`。

## 半监督深度学习

整体上，半监督深度学习会以包含标注数据和未标注数据混合的数据作为自己的训练集，其中一半后者在数量上会远远大于前者。
整体思路上，半监督深度学习会有三类架构：

 1. 先用未标注数据预训练模型后，再用标注数据进行微调，其中初始化方式可以：
    - 无监督预训练：所有数据逐层重构预训练，对网络每一层都做重构自编码，得到参数后用标注数据微调
    - 伪有监督预训练：通过标签生成都方法先把无标注数据打上标签再微调
 2. 先用标注数据训练网络，再根据网络中已有的深度特征来做半监督学习：其实就是一种伪有监督训练，先从有标签数据中获得特征，再利用这些特征对无标签数据进行分类，选出分类正确对数据加入训练，往复循环。这个过程可能会加入一定对无标签噪声干扰网络训练。
 3. 让网络`work in semi-supervised fashion`。上述方法其实还是会让网络运行在一种有监督的状态下，以下一些方法可以让算法真正处于一种半监督状态。

### 伪标签

#### 伪标签
网络先对无标签数据进行预测，将预测结果作为该数据的伪标签使用。代价函数可以为：
 
 <a href="https://www.codecogs.com/eqnedit.php?latex=L=\sum&space;_{m=1}^n&space;\sum&space;_{i=1}^C&space;L(y^m_i,f^m_i)&plus;\alpha&space;(t)&space;\sum_{m=1}^{{n}'}&space;\sum_{i=1}^C&space;L({y'_i}^m,{f'_i}^m)" target="_blank"><img src="https://latex.codecogs.com/svg.latex?L=\sum&space;_{m=1}^n&space;\sum&space;_{i=1}^C&space;L(y^m_i,f^m_i)&plus;\alpha&space;(t)&space;\sum_{m=1}^{{n}'}&space;\sum_{i=1}^C&space;L({y'_i}^m,{f'_i}^m)" title="L=\sum _{m=1}^n \sum _{i=1}^C L(y^m_i,f^m_i)+\alpha (t) \sum_{m=1}^{{n}'} \sum_{i=1}^C L({y'_i}^m,{f'_i}^m)" /></a>

分别是有标签数据的代价和无标签数据的代价之和。α(t)为相加的权重，决定着无标签数据代价在网络中的作用大小，一般初始时网络本身预测性能不佳，因此无法相信伪标签的真实性。所以α(t)会是满足一个从0开始的增长函数，现有论文中均使用了一种高斯型的爬升函数。[含代码分享](https://github.com/iBelieveCJM/pseudo_label-pytorch)

使用Ladder Networks的半监督学习。无监督预训练一般是用重构样本进行训练，其编码（学习特征）的目的是尽可能地保留样本的信息；而有监督学习是用于分类，希望只保留其本质特征，去除不必要的特征。LadderNet 通过 skip connection 解决这个问题，通过在每层的编码器和解码器之间添加跳跃连接（skip connection），减轻模型较高层表示细节的压力，使得无监督学习和有监督学习能结合在一起，并在最高层添加分类器，ladderNet 就变身成一个半监督模型。


### 一致性正则化

自监督学习中的Temporal Ensembling。Temporal Ensembling的目的是构造更好的伪标签，多个独立训练的网络的集成可取得更好的预测。πmodel和Temporal Ensembling的代价函数和伪标签方法类似，区别在于伪标签的第二部分只有无标签数据的代价，而Temporal Ensembling的代价函数中第二部分是面向所有半监督的数据。

Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results。模型成功的关键就在于target的质量，而提高target质量的方法无非是选择更好的样本噪声或找到一个更好的Teacher Model。这个方法选择了后一种手段，然后对模型的参数进行了`移动平均（weight-averaged）`，从而获得一个mean teacher model，再以此来构造更高质量的target。

### 混合方法




- - -

# Reference

- [Semi-Supervised Learning, Oliver Chapelle, et al](http://www.acad.bg/ebook/ml/MITPress-%20SemiSupervised%20Learning.pdf)
- [Phrase-Based & Neural Unsupervised Machine Translation, Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, Marc’Aurelio Ranzato](https://www.aclweb.org/anthology/D18-1549/)
- [Introduction to Semi-Supervised Learning Synthesis Lectures on Artificial Intelligence and Machine Learning, Xiaojin Zhu, Andrew B.Goldberg](https://morganclaypool.com/doi/abs/10.2200/S00196ED1V01Y200906AIM006)



