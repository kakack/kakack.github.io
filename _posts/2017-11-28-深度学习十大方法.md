---

layout: post
categories: [Deep Learning]
tags: [Machine Learning, Deep Learning,  Neural Network]

---

特别鸣谢：[The 10 Deep Learning Methods AI Practitioners Need to Apply](https://towardsdatascience.com/the-10-deep-learning-methods-ai-practitioners-need-to-apply-885259f402c1)

前言和客套我不写了，感觉都大同小异浪费我时间，直接上干货好了。也是对之前三篇文章中一些没有详细描述的算法概念做一个解释和补充

- - -

# 1 - 反向传播 Back-Propagation

![](http://ope2etmx1.bkt.clouddn.com/tdlm1.png)

Back-prop只是一种计算函数偏导数（或梯度）的方法，并具有函数组成的形式（如神经网络）。 当使用基于梯度的方法（梯度下降只是其中之一）来解决一个优化问题时，你需要在每次迭代时计算函数的梯度。

对于一个神经网络而言，目标函数事实上是以一种组合的形式存在。那需要如何来计算梯度？ 有两种常见的方式能做到这一点：（i）**分析型微分 Analytic differentiation**。 你已经知道了函数的形式，介休只需使用链式规则（基本演算）来导数。（ii）**使用有限差分进行近似微分  Approximate differentiation using finite difference**。 这种方法的计算量很大，因为函数评估的数量级是`O(N)`，其中N是参数的数量。与分析型微分相比，这是代价非常高的。所以，有限差分通常用于在调试时验证back-prop。

- - -

# 2 - 随机梯度下降  Stochastic Gradient Descent

描述梯度下降的一种直观的方式是，想象一条源于山顶的河流沿着山麓流下来的路径。梯度下降的目标正是河流努力实现的目标——即从山顶到达山的最底部（在山脚处）。

现在，假如山的地形可以保证在到达最终目的地（即山麓的最低点）之前，河流不需要完全停下来，那么这就是我们所希望达到的理想情况。在机器学习中，这就是说，我们已经找到了从初始点（山顶）开始的全局最小值（或最优值）的解。然而，可能因为地形的性质，使得路径上有几个陷坑，这可能会使得河流陷入坑中而不再流向真正的最低点山脚。在机器学习上，这种陷坑被称为局部最小值的解，然而这些解是不可取的，我们有很多方法可以避免这个问题（这不在我们当前讨论范围内）。

![](http://ope2etmx1.bkt.clouddn.com/tdlm2.png)

因此，梯度下降很容易会被卡在局部最小值，这取决于地形的性质（或ML中的函数）。但是，当你有一个特殊的山地形（形状像一个碗，在ML术语中称为凸函数），该算法始终能保证找到全局最优解。你可以再次想象有这样一条河流。这些类型的特殊的地形（也被称为凸函数）总是ML中优化的最理想状态。另外，梯度下降计算的结果也取决于你最初选择从哪里开始（即函数的初始值），不同的初始位置可能会导致不同的结果路径。同样，根据河流的下降的速度（即梯度下降算法的学习速率或步长），你也可能会以不同的方式到达最终目的地。这两个标准都可能影响你是否会陷入一个陷坑（局部最小）中。

- - -

# 3 - 学习率衰减 Learning Rate Decay 

在随机梯度下降优化过程中，调整学习率 learning rate 可以在一定程度上提高性能并减少训练时间。有时这被称为**学习速率衰减  learning rate annealing** 或**自适应学习速率 adaptive learning rates**。训练过程中最简单也最常用的学习速率调整是随着时间的推移而逐渐降低学习速度的技术。当使用较大的学习速率值时，它们在训练过程开始时可以有很明显的效果，然后在训练过程的后期，慢慢降低了学习速率，从而对训练进行更新。这不但能在前期快速获得较好的权重值，又能在后期慢慢将它们调整到最优。

![](http://ope2etmx1.bkt.clouddn.com/tdlm3.png)

两个流行和易于使用的学习率衰减方法：

- 逐步降低学习率。
- 在特定的时代使用断崖式巨大的降低学习速率然后再逐步降低。

- - -

# 4 - Dropout

具有大量参数的深度神经网络是一个非常强大的机器学习系统。但是，在这样的网络中一个严重的问题就是过拟合。大型网络的使用过程是很慢的，通过在测试阶段再去结合许多不同的大型神经网络的预测结果，是很难处理过拟合的。而dropout是解决这个问题的一种技巧。

![](http://ope2etmx1.bkt.clouddn.com/tdlm4.png)

关键的想法是在训练期间从神经网络中随机丢弃几个单位（连同他们的连接）。这可以防止单位被结合适应地太多。在训练期间，从指数级的不同“稀疏”网络中剔除几个样本。在测试时间，通过简单地使用具有较小权重的单个未被联系的网络来近似平均所有这些细化网络的预测的效果是很简单的。这显着减少了过拟合，并且比其他正则化方法都有了重大改进。Dropout已被证明可以提高神经网络在视觉监控学习任务，语音识别，文档分类和计算生物学上的性能，在许多基准数据集上都获得了最高水平的结果。

- - -

# 5 - 最大池化 Max Pooling

最大池化是一个基于样本的离散化过程。目标是对输入表示（图像，隐藏层的输出矩阵等）进行下采样，降低其维度，并允许对包含在分区域中的特征进行猜测。

![](http://ope2etmx1.bkt.clouddn.com/tdlm5.png)

这部分是通过提供表现的抽象形式来帮助防止过拟合。同时，它通过减少需要学习的参数数量来降低计算成本，并为内部表示提供基本变换一致性。最大池化是由初始表示的不重叠的子区域上的最大过滤器来完成的。

- - -

# 6 - 批量归一化  Batch Normalization

当然，包括深度网络的神经网络都需要仔细调整权重、初始化值和学习参数。 而批量归一化有助于他们减轻负担。

### 权重问题

- 无论权重的初始化如何，无论是随机的还是经验性的选择，它们都和学习到的权重相去甚远。 考虑一个小批量，在初始化阶段，在所需的特征激活上将会有许多异常值。

- 深层神经网络本身是非良置的，即初始层中的微小扰动都会导致后面层巨大的变化。

在反向传播过程中，这些现象会导致对梯度的分散，这意味着在学习权重以产生所需输出之前，梯度必须补偿异常值。这就导致需要额外的时间来完成汇合。

![](http://ope2etmx1.bkt.clouddn.com/tdlm6.png)

批量归一化通过不断正则化使这些梯度，使其从分散到正常值，并在小批量范围内流向共同目标（通过归一化）。

## 学习率问题

一般来说，学习率保持较低，使得只有一小部分的梯度能校正权重，原因是那些异常激活的梯度不应来影响正常的学习的激活。通过批量归一化，这些异常激活减少，因此可以使用更高的学习速度来加速学习过程。

- - -

# 7 - 长短记忆神经网络 Long Short-Term Memory

LSTM网络具有以下三个方面，使其与常规神经网络中的常见神经元不同：

- 它可以决定何时让输入进入神经元。
- 它可以决定何时记住上一个时间步骤中计算的内容。
- 它可以决定何时让输出传递到下一个时间戳。

LSTM的优点在于它可以根据当前的输入本身来决定所有这些内容。所以如果你看下面的图表：

![](http://ope2etmx1.bkt.clouddn.com/tdlm7.png)

当前时间戳的输入信号`x(t)`决定所有上述3个点。输入门决定点1，遗忘门决定点2，输出门决定点3。输入信号能够独自决定所有这三个点的取值。这是受到我们的大脑如何工作的启发，并且可以根据输入来处理突然出现的上下文切换。

- - -

# 8 — 跳跃图 Skip-gram

词嵌入模型的目标是为每个词汇项学习获得一个高维密集表示，其中嵌入向量之间的相似性代表了相应词语之间的语义或句法相似性。跳跃图Skip-gram是学习单词嵌入算法的模型。
跳跃图skip-gram模型（以及许多其他的词语嵌入模型）背后的主要思想如下：**如果两个词汇共享相似的上下文，则它们是相似的**。

换句话说，假设你有一个句子，就像“猫是哺乳动物”一样。如果你用“狗”而不是“猫”，这个句子还是一个有意义的句子。因此在这个例子中，“狗”和“猫”可以共享相同的上下文（即“是哺乳动物”）。

![](http://ope2etmx1.bkt.clouddn.com/tdlm8.png)

基于上述假设，可以考虑一个上下文窗口（一个包含k个连续项的窗口），然后跳过其中一个单词，让神经网络尝试根据除跳过内容的剩余所有项进行学习，并预测所跳过的项。如果两个词在一个大语料库中反复共享相似的语境，则这些词的嵌入向量将会是十分相近的向量。

- - -

# 9 - 词语的连续包 Continuous Bag Of Words

在自然语言处理问题中，我们希望学习将文档中的每个单词表示为一个数字的向量，使得出现在相似的上下文中的单词具有彼此接近的向量。在连续的单词模型中，我们的目标是能够使用围绕特定单词的上下文并预测特定单词。

![](http://ope2etmx1.bkt.clouddn.com/tdlm9.png)

我们通过在一个庞大的语料库中抽取大量的句子来做到这一点，每当我们看到一个单词时，我们就会选取其周围的词。然后，我们将上下文单词输入到一个神经网络，并预测在这个上下文中心的单词。

当我们有成千上万个这样的上下文单词和中心词时，我们就得到了一个神经网络数据集的实例。 我们训练神经网络，最后编码的隐藏层输出表示特定单词的嵌入。恰巧，当我们对大量的句子进行训练时，类似语境中的单词就会得到相似的向量。

- - -

# 10 - 变换学习 Transfer Learning

让我们考虑一下图像如何通过卷积神经网络。假设你有一个图像，你应用卷积，并得到像素的组合作为输出。假设他们是一组边缘。接着再次应用卷积，所以现在你的输出是边或线的组合。然后再次应用卷积，所以你的输出依然是线的组合等等。你可以把它看作是每一层寻找一个特定的模式。神经网络的最后一层往往会变得非常专业化。也许如果你在ImageNet上做过尝试，你的网络最后一层很可能就是寻找孩子，狗或飞机等等。你也可能会看到网络寻找眼睛或耳朵或嘴巴或轮子。

![](http://ope2etmx1.bkt.clouddn.com/tdlm10.png)

深度CNN中的每一层都逐渐建立起越来越高层次的特征表征。最后几层往往是专门针对你输入模型的任何数据。另一方面，早期的层会更为通用，在一大类图片中往往会巨有许多简单的图形模式。

转移学习就是当你在一个数据集上训练一个CNN时，去掉最后一个层，然后再在不同的数据集上重新训练最后一层的模型。直观地说，你可以在原有模型的基础上，重新训练模型以识别不同的高级功能。因此，训练时间会减少很多，所以当你没有足够的数据或者训练模型需要消耗太多的资源时，转移学习是一个非常有用的工具。

- - -

# Reference

本文仅显示这些方法的一般概述。 我建议阅读下面的文章以获得更详细的解释：

- Andrew Beam’s [“Deep Learning 101”](http://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html)
- Andrey Kurenkov’s [“A Brief History of Neural Nets and Deep Learning”](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/)
- Adit Deshpande’s [“A Beginner’s Guide to Understanding Convolutional Neural Networks”](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)
- Chris Olah’s [“Understanding LSTM Networks”](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- Algobean’s [“Artificial Neural Networks”](https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/)
- Andrej Karpathy’s [“The Unreasonable Effectiveness of Recurrent Neural Networks”](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
