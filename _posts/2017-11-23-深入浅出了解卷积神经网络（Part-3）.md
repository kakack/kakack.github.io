---

layout: post
categories: [Deep Learning]
tags: [Machine Learning, Deep Learning,  Neural Network]

---

特别鸣谢：[The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)

![](http://ope2etmx1.bkt.clouddn.com/Cover3rd.png)


- - -

# 简介

[Links to Part1](http://kakack.github.io/2017/11/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E4%BA%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Part-1/)

[Links to Part2](http://kakack.github.io/2017/11/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E4%BA%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Part-2/)

在这篇文章中，我们将总结计算机视觉和卷积神经网络领域的许多新的重要发展。我们将回顾过去五年来发表的一些最重要的论文，并讨论它们为什么如此重要。所列举的前半部分（AlexNet to ResNet）涉及一般网络架构的进步，而后半部分则是其他子领域的一些有趣的论文。


 - - -

# [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)(2012)

这一开始是这样的（虽然有些人可能会说，1998年的Yann LeCun的论文才是真正的开创性的刊物）：这篇名为“深度卷积网络的ImageNet分类”的文章总共被引用了6184次，被广泛认为是该领域最有影响力的发表物之一。 Alex Krizhevsky，Ilya Sutskever和Geoffrey Hinton创建了一个“大规模的深度卷积神经网络”，用于赢得2012年ILSVRC（ImageNet大规模视觉识别挑战）。对于那些不熟悉的人来说，这个竞赛可以被认为是计算机视觉年度奥运会，来自世界各地的团队参与竞赛，角逐谁拥有最好的计算机视觉模型，比赛的内容有分类，定位，检测等等。2012年标志着CNN被用来实现top 5达到15.4％的测试错误率的第一年（top 5的错误率是在给定图像的情况下，该模型没能输出top 5的正确标签的比例）。紧随其后的成绩是26.2％的错误率，所以这是一个惊人的改进，几乎震惊了计算机视觉社区。可以这么说，CNN在这个竞争中从此变成了家喻户晓的名字。

在论文中，研究小组讨论了网络的架构（称为AlexNet）。与现代架构相比，他们使用相对简单的布局。网络由5个conv层组成：一个max-pooling层，一个dropout层和3个全连接层。他们设计的网络被用于1000种可能类别的分类。

![](http://ope2etmx1.bkt.clouddn.com/AlexNet.png)

## 主要观点

- 在ImageNet数据上训练网络，其中包含一千五百万张被标记成两万两千个类别的图像。
- 针对非线性函数使用ReLU（发现能有效减少训练时间，因为ReLU比传统tanh函数快几倍）。
- 使用数据增强技术，包括图像转换，水平反射和补丁提取。
- 使用Dropout层，以解决模型过拟合的问题。
- 使用批量随机梯度下降训练模型，其中动量和权重的衰减有特定的值。
- 在两个GTX 580 GPU上训练**五到六天**。

## 重要性

Krizhevsky，Sutskever和Hinton于2012年开发的神经网络是CNN在计算机视觉领域的一次派对。这是历史上第一次有模型能在困难的ImageNet数据集上表现得如此之好。利用这些今天仍在使用的如数据增加和dropout技术，本文真正说明了CNN的好处，并在实际竞争中发挥出了创纪录的表现。

 - - -

# [ZF Net](http://arxiv.org/pdf/1311.2901v3.pdf)(2013)

随着AlexNet于2012年的大放异彩，提交给ILSVRC 2013的CNN模型数量大幅增加。当年的比赛获胜者是来自纽约大学的Matthew Zeiler和Rob Fergus建立的网络，被命名为ZF Net的这个模型实现了仅有11.2％的错误率。这种架构相对于以前的AlexNet结构被调整得更加优秀，但仍然提出了一些关于提高性能的非常关键的想法。这篇论文之所以十分重要的另一个原因，是作者花了很多时间来解释ConvNets背后的直观理解，并展示了如何正确地将过滤器和权重可视化。

在这篇题为“可视化和理解卷积神经网络”的论文中，Zeiler和Fergus首先讨论了这一观点，即重新对CNN产生兴趣是由于大型训练集的可访问性以及随着GPU的使用而增加的计算能力。他们还谈到了研究人员对这些模型的内在机制的有限的知识，他说如果没有这种洞察力，“在开发更好的模型的过程中就会不断碰壁遇到错误”。虽然我们现在比三年前对模型有了更好的了解，但对于很多研究人员来说，这仍然是一个问题！本文的主要贡献是略微修改了AlexNet模型的细节，以及给出了一种非常有趣的可视化特征映射方式。

![](http://ope2etmx1.bkt.clouddn.com/zfnet.png)

## 主要观点

- 一个与AlexNet非常相似的架构，除了一些小的修改。
- AlexNet训练了1500万张图片，而ZF Net仅训练了130万张图片。
- ZF Net没有在第一层使用`11x11`大小的过滤器（这是AlexNet实现的），而是使用了尺寸为`7x7`的过滤器和更小的步幅值。这种修改背后的原因是，第一个conv层中的较小的过滤器尺寸有助于在输入体量中保留大量的原始像素信息。过滤大小为`11x11`被证明是跳过了大量的相关信息，特别是在第一个conv层上。
- 随着网络的发展，我们也看到了所使用的过滤器数量的增加。
- 在激活函数上使用ReLUs，在损失函数上使用交叉熵损失，以及使用批次随机梯度下降来进行训练。
- 在GTX 580 GPU上训练了**十二天**。
- 开发了一种名为`Deconvolutional Network`的可视化技术，可以帮助检查不同的特征激活及其与输入空间的关系。因为它将特征映射到像素（与卷积层相反），所以称为“deconvnet（反向卷积）”。

## DeConvNet

DeConvNet如何工作背后的基本思想是，在训练有素的CNN的每一层，你都附加一个“去卷积deconvnet”，也就是说它有一条反向映射回到图像像素的路径。输入图像被输入到CNN中，并在每个级别计算激活，这是正向传递。现在，假设我们想要检查第四个conv层中某个特征的激活。我们将存储这个特征映射的激活，但是将该层中的所有其他激活设置为0，然后将该特征映射作为输入传递给去卷积deconvnet。 这个deconvnet和原来的CNN有相同的过滤器。然后，这个输入经过一系列的逆向池化unpool（池化maxpooling的逆操作），整流和滤波操作，直到达到输入空间。

整个过程背后的原因是我们想要检查哪种类型的结构激发给定的特征映射。我们来看看第一层和第二层的可视化。

![](http://ope2etmx1.bkt.clouddn.com/deconvnet.png)

就像我们在[Part1](http://kakack.github.io/2017/11/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E4%BA%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Part-1/)中已经讨论过的那样，ConvNet的第一层总是那些勇于探测低等级特征的，例如简单的边缘或者特定例子中的颜色等。我们可以在第二层中看到，我们有越来越多的圆形特征会被探测到。让我们继续来看第3、4、5层。


![](http://ope2etmx1.bkt.clouddn.com/deconvnet2.png)

这些图层显示了更多更高级别的功能，如探测狗脸或鲜花。 有一点需要注意的是，在第一个conv层之后，我们通常有一个对图像进行下采样的池（例如，将一个`32x32x3`的数据体量volumes变成一个`16x16x3`的体量）。这样做的效果是，在第二层中可以探测到在原始图像中更广泛的范围。 有关deconvnet或一般论文的更多信息，请查看Zeiler的[presentation](https://www.youtube.com/watch?v=ghEmQSxT6tw)的主题。

## 重要性

ZF Net不仅在2013年的竞争中胜出，而且在CNN的运作方面也提供了很好的直观理解，并说明了更多提高性能的方法。所描述的可视化方法不仅有助于解释CNN的内部工作原理，而且还提供了一些对网络体系结构进行改进的建议。迷人的deconv可视化方法和闭塞实验使得这篇论文成为了我个人最喜欢的论文之一。

- - -

# [VGG Net](http://arxiv.org/pdf/1409.1556v6.pdf)（2014）

VGG Net，这个2014年创建的模型（不是ILSVRC 2014的获奖者）依靠它7.3％的错误率为我们展现了它简单和深入两大特性。这是牛津大学的Karen Simonyan和Andrew Zisserman创建的一个19层的CNN，严格使用了`3x3`的过滤器，填充padding为1，最大池化层maxpooling layer为`2x2`，步幅stride为2。

![](http://ope2etmx1.bkt.clouddn.com/VGGNet.png)

## 主要观点

- 仅使用`3x3`大小的过滤器，这与应用于第一层中的AlexNet `11x11`过滤器和ZF Net的`7x7`过滤器完全不同。作者的推理是，两个`3x3`的conv层的组合具有`5x5`的有效感受域。这反过来可以模拟一个更大的过滤器，同时又能保持较小的过滤器尺寸的好处。其中一个好处是减少了参数的数量。此外，有了两个conv层，我们可以使用两个ReLU层而不是一个。
- 背靠背的三个conv的层可以实现一个`7x7`尺寸的接受领域的效果。
- 随着每层输入体量的空间尺寸减小（conv和pool层输出的结果），体量的深度也会随着过滤器数量的增加而增加。
- 有趣的是，过滤器数量在每个maxpool层之后都会增加一倍。这强化了缩小空间维度的想法，但增加了深度。
- 在图像分类和定位任务上效果很好。作者使用了一种定位方法作为回归（详见本篇[论文](http://arxiv.org/pdf/1409.1556v6.pdf)的第10页）。
- 使用Caffe工具箱（Caffe Toolbox）建立模型。
- 在训练过程中使用比例抖动（scale jittering）作为一种数据增强技术。
- 在每个conv层之后使用ReLU层，并使用批梯度下降进行训练。
- 在4个Nvidia Titan Black GPU上训练**2到3周**。

## 重要性

VGG Net是我心目中最有影响力的论文之一，因为它进一步强化了**卷积神经网络必须具有深层网络才能使视觉数据的分层表示能顺利实现**的观点。

- - -

# [GoogLeNet](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)(2015)

刚才我们谈到了简单网络架构的概念。2015年，谷歌推出了Inception模块。GoogLeNet是一个22层的CNN，是2014年ILSVRC的赢家，top 5的错误率为6.7％。据我所知，这是第一个真正不再沿用在一个顺序结构中简单地堆叠conv和pooling层叠的一般方法的CNN。这篇论文的作者还强调，这个新模型对内存和电源使用有一些考虑（重要的是，我有时也会忘记：堆叠所有这些层并添加大量的过滤器会带来计算和存储成本，以及增加过拟合的可能）。

![](http://ope2etmx1.bkt.clouddn.com/GoogleNet.gif)

![](http://ope2etmx1.bkt.clouddn.com/GoogLeNet.png)

## Inception 模块

当我们第一次接触GoogLeNet的时候，我们立即会注意到，不是所有事件都是像之前的架构那样串行顺序发生的。我们的网络中有部分功能是并行进行的。

![](http://ope2etmx1.bkt.clouddn.com/GoogLeNet2.png)

这个方块被称为Inception模块，让我们仔细看看这是由什么组成的：

![](http://ope2etmx1.bkt.clouddn.com/GoogLeNet3.png)

最下面的绿色框是我们的输入，最上面的是模型的输出（把这张照片右转90度可以让你看到上一张显示整个网络的模型的图片）。基本上，在传统的ConvNet的每一层，你必须选择是否有池操作pooling operation或卷积操作conv operation（也有筛选器大小的选择）。 Inception模块允许你做的是并行执行所有这些操作。事实上，这正是作者提出的最“朴素”的想法。

![](http://ope2etmx1.bkt.clouddn.com/GoogLeNet4.png)

那么现在为什么这行不通呢？因为这会导致出现太多的输出。我们最终会得到一个非常大的输出体量的深度频道。作者解决这个问题的方法是在`3x3`和`5x5`层之前添加`1x1`的conv操作。 `1×1`卷积（或网络层中的网络）提供了降维的方法。例如，假设你的输入体量为`100x100x60`（这不一定是图像的尺寸，只是网络任何层的输入）。应用20个`1x1`卷积滤波器可以让你将体量减小到`100x100x20`。这意味着`3x3`和`5x5`的卷积将不会有很大的处理量。这可以被认为是“特征池化pooling of features”，因为我们正在减小体量的深度，类似于我们如何使用普通的maxpooling层来减小高度和宽度的尺寸。另外值得注意的是这些`1x1`的conv层后面跟着的是不能被伤害到的ReLU units（参见Aaditya Prakash关于1x1卷积效果的[更多信息](http://iamaaditya.github.io/2016/03/one-by-one-convolution/)）。看看这个[视频](https://www.youtube.com/watch?v=VxhSouuSZDY)最后的过滤器连接的可视化。

你可能会问自己：“这个架构如何给我们提供帮助？”好，如果你有一个由网络层组成的网络，一个中等大小的卷积滤波器，一个大尺寸的卷积过滤器和一个池化操作pooling operation。网络中的网络能够提取非常详细的关于体量细节的信息，而`5x5`滤波器能够覆盖输入的较大接受区域，从而能够提取其信息。你也有一个池化操作，用来减少空间大小和预防过拟合。最重要的是，每个conv层之后都有ReLU，这有助于提高网络的非线性。基本上，网络能够执行这些不同操作的功能，同时仍然保持计算上的一致性。本文也给出了更多的高层次推理，涉及稀疏性和密集联系等主题（请阅读[论文](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)的第3和第4部分，但对我来说还不完全清楚，但如果有人有任何见解，我很乐意在评论中交流！）。

## 主要观点

- 在整个架构中使用了9个Inception模块，共有100多层。确实非常深。
- 没有使用全连接层，而使用平均池average pool来代替，从`7x7x1024`体量到`1x1x1024`体量。这节省了大量的参数。
- 使用比AlexNet少12倍的参数。
- 在测试过程中，创建多个相同的图像，并将其输入网络，并在最终结果上生成平均softmax概率。
- 使用R-CNN中的概念（我们将在后面讨论）作为检测模型。
- Inception模块有更新的版本（版本6和7）。
- 在若干个高端GPU上进行一周的**训练**。

## 重要性

GoogLeNet是第一个引入了“CNN层次并不总是必须依次叠加”概念的模型之一。 在Inception模块之后，作者们表示，在层次结构上的创造性可以提高性能和计算效率。这篇论文确实为我们在未来几年可以看到的一些令人惊叹的架构奠定了基础。

那么让我们进一步看看更深层次的内容。

- - - 

# [Microsoft ResNet](https://arxiv.org/pdf/1512.03385v1.pdf)(2015)

想象一下深度的CNN架构，接下来，将层数增加一倍，再增加几个层次，但仍然不如微软亚洲研究院在2015年年底提出的ResNet架构那么深。ResNet是一个新的152层网络架构，这是一个新的记录。这是一个可以进行分类，检测和定位的令人难以置信的架构。除了层数方面的新纪录，ResNet以3.6％的惊人错误率赢得了ILSVRC 2015（根据他们的技术和专业知识，人类通常徘徊在5-10％的错误率。可以查阅Andrej Karpathy的[文章](http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/)，里面阐述了他在ImageNet Challenge上与ConvNet竞争的经验）。


![](http://ope2etmx1.bkt.clouddn.com/ResNet.gif)

## 残差模块 Residual Block

残差块背后的想法是，当你的输入`x`通过`conv-relu-conv`系列层。这会给你一些`F(x)`。 该结果然后被添加到原始输入`x`。 我们称之为`H(x)= F(x)+x`。 在传统的CNN中，你的`H(x)`就等于`F(x)`。所以，我们不是只计算这个变换（直接从`x`到`F(x)`），我们需要计算需要往输入`x`上添加的部分——`F(x)`。 基本上，下面显示的迷你模块正在计算`delta`或着对原始输入`x`的轻微改变以获得稍微改变的结果表示（当我们考虑传统的CNN时，我们从`x`到`F(x)`，这是一个不保留关于原始x的任何信息的表示）。作者认为，“优化残差映射比优化原始的、未引用的映射更容易”。

![](http://ope2etmx1.bkt.clouddn.com/ResNet.png)

残差模块可能会有效的另一个原因在于，当逆向传递的过程中，梯度会随着图很轻易地流动，因为我们有额外添加的分割梯度的操作。

## 主要观点

- “非常深” - Yann LeCun。
- 152层。
- 有意思的是，在仅仅前两层之后，空间大小从`224×224`的输入体量被压缩到`56×56`体量。
- 作者声称，普通网络中层次的增加会导致更高的训练和测试错误（[本文](https://arxiv.org/pdf/1512.03385v1.pdf)中的图1）。
- 该研发组织尝试了一个1202层的网络，但测试的准确性较低，大概是由于过配合导致。
- 在8GPU机器上训练*2到3周*。

## 重要性

3.6％的错误率，这本身就有足够的说服力。ResNet模型是我们目前拥有的最好的CNN架构，是残留学习理念的一个伟大创新。由于自2012年以来每年的错误率都在下降，我对今年错误率是否会继续下降表示怀疑。我相信我们已经到了即便继续堆叠更多层但结果不会导致大幅提升的地步。但是，相信接下去肯定会继续诞生像我们在过去两年看到的那样有创意的新架构。

 [ResNets inside of ResNets](http://arxiv.org/pdf/1608.02908.pdf)

- - -

# 基于域的CNN：Region Based CNNs ([R-CNN](https://arxiv.org/pdf/1311.2524v5.pdf) - 2013, [Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf) - 2015, [Faster R-CNN](http://arxiv.org/pdf/1506.01497v3.pdf) - 2015)

有些人可能会认为，R-CNN的出现比以前关于新网络体系结构的任何论文都有更大的影响力。 R-CNN的第一篇论文被引用了1600多次，加州大学伯克利分校的Ross Girshick和他的团队实现了计算机视觉领域最有影响力的创举之一。正如其标题所说的，Fast R-CNN和Faster R-CNN正在使模型更快更好地适用于现代物体检测任务。

R-CNNs的目的是解决目标检测的问题。当给定一个图像，我们希望能够绘制所有对象的边界框。该过程可以分为两个通用的组成部分，区域划分建议步骤和分类步骤。

作者指出，这个模型适合任何未知类区域的检测。选择性搜索是特定用于RCNN的一种方法。选择性搜索执行生成具有包含对象的可能性最高的2000个不同区域的函数。在我们提出了一系列的区域建议后，这些建议被“扭曲warped”成一个可以输入到训练完毕的CNN（在这种情况下是AlexNet）的图像大小，其中这个CNN在每个区域都提取得到了特征向量。这个矢量然后被当作一组线性SVM的输入，这些线性SVM每一个都被训练用于判断一个类，并最终输出这个分类。矢量也被送入边界框回归器以获得其最准确的坐标。

![](http://ope2etmx1.bkt.clouddn.com/rcnn.png)

然后使用非最大值抑制Non-maxima suppression来抑制彼此具有显着重叠的边界框。

## Fast R-CNN

由于3个主要问题，对原始模型进行了改进。训练经历了多个阶段（ConvNets到SVM到边界框回归），计算成本很高，并且非常慢（RCNN每个图像需要53秒）。Fast R-CNN可以通过共享不同提议之间的conv层的计算结果，并交换生成区域建议的顺序和运行CNN，来解决速度问题。 在这个模型中，图像首先被投入通过一个ConvNet，从ConvNet的最后一个特征映射获得区域建议的特征（更多细节请查看本文2.1节），最后我们还有全连接层来完成我们的回归和分类。

![](http://ope2etmx1.bkt.clouddn.com/FastRCNN.png)

## Faster R-CNN

Faster R-CNN正在努力克服R-CNN和Fast R-CNN所展现出的过于复杂的训练渠道所带来的问题。 作者在最后的卷积层之后插入区域建议网络（region proposal network，RPN）。 这个网络能够查看最后的卷积特征映射，并从中产生区域提议。从那个阶段开始，接着使用与R-CNN相同的流水线管道（ROI池，FC，然后是分类和回归开始）。

![](http://ope2etmx1.bkt.clouddn.com/FasterRCNN.png)

## 重要性

能够确定图像中的特定对象是一回事，但是能够确定对象的确切位置是计算机知识的一个巨大的跳跃。 Faster R-CNN已经成为当今物体检测程序的标准。

- - -

# [生成对抗网络 Generative Adversarial Networks](https://arxiv.org/pdf/1406.2661v1.pdf) (2014)

[据Yann LeCun介绍](https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning)，这些网络可能是下一个巨大飞跃。 在谈论这篇文章之前，我们来谈谈一些关于具有对抗性（adversarial）的例子。例如，让我们设想一个训练好的CNN在ImageNet数据上运行结果良好。让我们有一个图像作为例子，并应用一个摄动干扰，或稍作修改，使预测误差最大化。因此，预测的对象类别被改变，而图像本身与没有摄动的图像相比而言，看上去是相同的。从最高层面来看，具有对抗性的例子本质上就是成功欺骗了ConvNets的图像。

![](http://ope2etmx1.bkt.clouddn.com/Adversarial.png)

对抗性的例子（[论文](http://arxiv.org/pdf/1312.6199v4.pdf)）肯定使很多研究者感到惊讶，并迅速成为一个有趣的话题。现在让我们来谈谈如何生成对抗网络。我们来考虑两个模型，一个生成模型和一个判别模型。判别模型的任务是确定给定图像是否看起来很自然（来自数据集的图像），还是看起来像是人为创建的。生成器的任务是创建图像，使鉴别器得到训练，产生正确的输出。这可以被认为是一个零和博弈（Zero-sum）或极大极小（minimax）之类的双人游戏。本文所给出的的类比是这样的：生成模型就像“伪造者团队，试图制造和使用假货币”，而判别模型就像“警察试图检测假货币”。发生器试图欺骗鉴别器，而鉴别器试图不被发生器愚弄。随着模型的训练，这两种方法都得到了改进，直到“伪造品与真品无法区分”的程度。

## 重要性

这听起来似乎很简单，但为什么我们要去关心这些网络？正如Yan LeCun在Quora文章中指出的那样，鉴别者现在已经能意识到“数据的内在表现”，因为它已经被训练去了解数据集中真实图像与人工创建图像之间的差异。因此，它可以在CNN中被当做一个特征提取器来使用。另外，你可以创建非常酷的，对我而言十分自然的人造图像（[link](http://soumith.ch/eyescream/)）。

 - - -

# [生成图像描述Generating Image Descriptions](https://arxiv.org/pdf/1412.2306v2.pdf) (2014)

当你将CNN和RNN结合起来的时候会发生什么（不，，对不起，你并没有得到R-CNN），但是你确实会得到一个非常了不起的应用。这篇论文由Andrej Karpathy（我个人最喜爱的作者之一）和Fei-Fei Li撰写，将CNN和双向RNN（递归神经网络Recurrent Neural Networks）结合起来，生成不同图像区域的自然语言描述。通常，该模型能够获取图像，并输出以下类似信息：

![](http://ope2etmx1.bkt.clouddn.com/Caption.png)

这真是不可思议！我们来看看这与正常的CNN相比如何。使用传统的CNN，训练数据中每个图像都有一个明确的标签。本文中描述的模型有训练样例，每个图像都有一个句子（或标题）来描述。这种类型的标签被称为弱标签，其中句子片段指图像的（未知）部分。通过这个训练数据，一个深度神经网络可以“推断句子的片段和他们描述的区域之间的潜在校准关系”（引自论文原文）。另一个神经网络将图像作为输入，并在文本中生成描述。让我们分别看看这两个组件——校准和生成。

## 校准模型Alignment Model

这部分模型的目低是能够校准视觉图像和文本数据（图像及其语句描述）。这个模型通过接收一个图像和一个句子作为输入来进行工作，输出是一个关于匹配程度的得分（现在，Karpathy提到了另一个不同的[论文](https://arxiv.org/pdf/1406.5679v1.pdf)，它涉及了这个原理的具体细节，这个模型同时在兼容和不兼容的图像与句子组合上进行训练）。

现在让我们考虑图像的表示。第一步是将图像输入到R-CNN中以检测单个物体。这个R-CNN在ImageNet数据上进行了训练。顶部十九个（加上原始图像）对象区域被嵌入到500维空间。现在我们有20个不同的500维矢量（在本文中用v表示）用于每个图像。于是我们就有了关于图像的信息。现在，我们需要关于这个句子的信息。我们需要将把语句嵌入到这个相同的多模态空间中。这是通过使用双循环归神经网络（bidirectional recurrent neural network）完成的。从最高层次来看，这用于阐明给定句子中单词的上下文的信息。由于这些关于图片和句子的信息都在同一个空间，所以我们可以通过计算内积inner product来表示相似度。

## 生成模型Generation Model

校准模型的主要目的是创建一个数据集，其中有一组图像区域（由RCNN找到）和相应的文本（多亏了BRNN）。现在，生成模型将从该数据集中学习，以生成给定图像的描述。该模型需要一个图像，并将其提供给CNN。由于全连接层的输出成为了另一个RNN的输入，所以softmax层被忽略了。对于那些不熟悉RNN的人来说，它们的功能是基本上是产生获得句子中不同单词的概率分布（RNN也需要像CNN一样进行训练）。

![](http://ope2etmx1.bkt.clouddn.com/GeneratingImageDescriptions.png)

## 重要性

对我来说有趣的是，使用这些看似不同的RNN和CNN模型来创建一个非常有用的应用程序，并能狗结合计算机视觉和自然语言处理两大重要领域。它为之后处理跨越不同领域的任务时，如何使计算机和模型变得更加智能化打开了新的思路。

- - -

# [空间变换神经网络 Spatial Transformer Networks](https://arxiv.org/pdf/1506.02025.pdf) (2015) 

最后，让我们来看看这个领域最近的一篇文章。这篇文章是一年多前在Google Deepmind上发表的。主要贡献是引入了一个空间变换Spatial Transformer模块。其基本思想是，该模块以某种方式转换输入图像，以便后续图层能更容易地被进行分类。在不改变主CNN本身体系结构的情况下，作者担心的是如何在将图像输入特定的conv层之前对图像进行更改。这个模块希望纠正的两件事情是姿态归一化pose normalization（对象被倾斜或缩放的场景）和空间注意力spatial attention（在拥挤的图像中注意正确的对象）。对于传统的CNN，如果你想让你的模型对于不同尺度和旋转的图像而言是不变的，那么你需要大量的训练样例才能正确训练模型。我们来详细了解这个变换模块如何帮助解决这个问题。

在传统CNN模型中，处理空间不变性的实体是最大池化层maxpooling layer。 这一层背后的直观推论是，一旦我们知道某个特定特征位于原始输体入量中（高激活值的任何地方），它的确切位置就不如他与其它特征之间的相对位置那么重要。而这种新的空间转换器是动态的，它将为每个输入图像产生不同的行为（不同的失真/变换）。 它不像传统的最大池化层Maxpool那样是很简单并预先定义好的。让我们来看看这个变压器模块是如何工作的。该模块包括：

- 接入输入体量并输出应该被应用到的空间变换的参数的定位网络。对于仿射变换，参数或θ可以是6维的。
- 创建一个采样网格，这个网格是由定位网络中产生的仿射变换（θ）扭曲正常的网格而得到的结果。
- 采样器，其目的是执行输入特征映射的变形。

![](http://ope2etmx1.bkt.clouddn.com/SpatialTransformer.png)

这个模块在任何时候都可以放入到CNN中，基本上可以帮助网络来学习如何在训练过程中以最小化损失函数的方式来转换特征映射。

![](http://ope2etmx1.bkt.clouddn.com/SpatialTransformer2.png)

## 重要性

这篇论文引起了我注意的主要原因是，CNN的改进不一定要由网络架构的剧烈变更引起。我们不需要再去创建下一个ResNet或Inception模块。本文实现了对输入图像进行仿射变换的简单思想，以帮助模型在处理平移、缩放和旋转情况下保持自身的不变性。对于那些感兴趣的读者，这里有一个Deepmind的[视频](https://drive.google.com/file/d/0B1nQa_sA3W2iN3RQLXVFRkNXN0k/view)，是一个描述CNN上的空间变换器模块工作的动画，同时欢迎加入到Quora的[讨论组](https://www.quora.com/How-do-spatial-transformer-networks-work)来。

- - -

# 结语

以上便是关于ConvNet的三部分介绍，欢迎大家评论中交流各自心得体会。如果想了解更多相关概念，我推荐大家去看Stanford CS 231n的课程。谢谢！
