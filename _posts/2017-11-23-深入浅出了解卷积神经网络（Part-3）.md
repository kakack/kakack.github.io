---

layout: post
categories: [Deep Learning]
tags: [Machine Learning, Deep Learning,  Neural Network]

---

特别鸣谢：[The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)

![](http://ope2etmx1.bkt.clouddn.com/Cover3rd.png)


- - -

# 简介

[Links to Part1](http://kakack.github.io/2017/11/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E4%BA%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Part-1/)

[Links to Part2](http://kakack.github.io/2017/11/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E4%BA%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Part-2/)

在这篇文章中，我们将总结计算机视觉和卷积神经网络领域的许多新的重要发展。我们将回顾过去五年来发表的一些最重要的论文，并讨论它们为什么如此重要。所列举的前半部分（AlexNet to ResNet）涉及一般网络架构的进步，而后半部分则是其他子领域的一些有趣的论文。


 - - -

 # [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)(2012)

这一开始是这样的（虽然有些人可能会说，1998年的Yann LeCun的论文才是真正的开创性的刊物）：这篇名为“深度卷积网络的ImageNet分类”的文章总共被引用了6184次，被广泛认为是该领域最有影响力的发表物之一。 Alex Krizhevsky，Ilya Sutskever和Geoffrey Hinton创建了一个“大规模的深度卷积神经网络”，用于赢得2012年ILSVRC（ImageNet大规模视觉识别挑战）。对于那些不熟悉的人来说，这个竞赛可以被认为是计算机视觉年度奥运会，来自世界各地的团队参与竞赛，角逐谁拥有最好的计算机视觉模型，比赛的内容有分类，定位，检测等等。2012年标志着CNN被用来实现top 5达到15.4％的测试错误率的第一年（top 5的错误率是在给定图像的情况下，该模型没能输出top 5的正确标签的比例）。紧随其后的成绩是26.2％的错误率，所以这是一个惊人的改进，几乎震惊了计算机视觉社区。可以这么说，CNN在这个竞争中从此变成了家喻户晓的名字。

在论文中，研究小组讨论了网络的架构（称为AlexNet）。与现代架构相比，他们使用相对简单的布局。网络由5个conv层组成：一个max-pooling层，一个dropout层和3个全连接层。他们设计的网络被用于1000种可能类别的分类。

![](http://ope2etmx1.bkt.clouddn.com/AlexNet.png)

## 主要观点

- 在ImageNet数据上训练网络，其中包含一千五百万张被标记成两万两千个类别的图像。
- 针对非线性函数使用ReLU（发现能有效减少训练时间，因为ReLU比传统tanh函数快几倍）。
- 使用数据增强技术，包括图像转换，水平反射和补丁提取。
- 使用Dropout层，以解决模型过拟合的问题。
- 使用批量随机梯度下降训练模型，其中动量和权重的衰减有特定的值。
- 在两个GTX 580 GPU上训练**五到六天**。

## 重要性

Krizhevsky，Sutskever和Hinton于2012年开发的神经网络是CNN在计算机视觉领域的一次派对。这是历史上第一次有模型能在困难的ImageNet数据集上表现得如此之好。利用这些今天仍在使用的如数据增加和dropout技术，本文真正说明了CNN的好处，并在实际竞争中发挥出了创纪录的表现。

 - - -

 # [ZF Net](http://arxiv.org/pdf/1311.2901v3.pdf)(2013)

随着AlexNet于2012年的大放异彩，提交给ILSVRC 2013的CNN模型数量大幅增加。当年的比赛获胜者是来自纽约大学的Matthew Zeiler和Rob Fergus建立的网络，被命名为ZF Net的这个模型实现了仅有11.2％的错误率。这种架构相对于以前的AlexNet结构被调整得更加优秀，但仍然提出了一些关于提高性能的非常关键的想法。这篇论文之所以十分重要的另一个原因，是作者花了很多时间来解释ConvNets背后的直观理解，并展示了如何正确地将过滤器和权重可视化。

在这篇题为“可视化和理解卷积神经网络”的论文中，Zeiler和Fergus首先讨论了这一观点，即重新对CNN产生兴趣是由于大型训练集的可访问性以及随着GPU的使用而增加的计算能力。他们还谈到了研究人员对这些模型的内在机制的有限的知识，他说如果没有这种洞察力，“在开发更好的模型的过程中就会不断碰壁遇到错误”。虽然我们现在比三年前对模型有了更好的了解，但对于很多研究人员来说，这仍然是一个问题！本文的主要贡献是略微修改了AlexNet模型的细节，以及给出了一种非常有趣的可视化特征映射方式。

![](http://ope2etmx1.bkt.clouddn.com/zfnet.png)

## 主要观点

- 一个与AlexNet非常相似的架构，除了一些小的修改。
- AlexNet训练了1500万张图片，而ZF Net仅训练了130万张图片。
- ZF Net没有在第一层使用`11x11`大小的过滤器（这是AlexNet实现的），而是使用了尺寸为`7x7`的过滤器和更小的步幅值。这种修改背后的原因是，第一个conv层中的较小的过滤器尺寸有助于在输入体量中保留大量的原始像素信息。过滤大小为`11x11`被证明是跳过了大量的相关信息，特别是在第一个conv层上。
- 随着网络的发展，我们也看到了所使用的过滤器数量的增加。
- 在激活函数上使用ReLUs，在损失函数上使用交叉熵损失，以及使用批次随机梯度下降来进行训练。
- 在GTX 580 GPU上训练了**十二天**。
- 开发了一种名为`Deconvolutional Network`的可视化技术，可以帮助检查不同的特征激活及其与输入空间的关系。因为它将特征映射到像素（与卷积层相反），所以称为“deconvnet（反向卷积）”。

## DeConvNet

DeConvNet如何工作背后的基本思想是，在训练有素的CNN的每一层，你都附加一个“去卷积deconvnet”，也就是说它有一条反向映射回到图像像素的路径。输入图像被输入到CNN中，并在每个级别计算激活，这是正向传递。现在，假设我们想要检查第四个conv层中某个特征的激活。我们将存储这个特征映射的激活，但是将该层中的所有其他激活设置为0，然后将该特征映射作为输入传递给去卷积deconvnet。 这个deconvnet和原来的CNN有相同的过滤器。然后，这个输入经过一系列的逆向池化unpool（池化maxpooling的逆操作），整流和滤波操作，直到达到输入空间。

整个过程背后的原因是我们想要检查哪种类型的结构激发给定的特征映射。我们来看看第一层和第二层的可视化。

![](http://ope2etmx1.bkt.clouddn.com/deconvnet.png)

就像我们在[Part1](http://kakack.github.io/2017/11/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E4%BA%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Part-1/)中已经讨论过的那样，ConvNet的第一层总是那些勇于探测低等级特征的，例如简单的边缘或者特定例子中的颜色等。我们可以在第二层中看到，我们有越来越多的圆形特征会被探测到。让我们继续来看第3、4、5层。


![](http://ope2etmx1.bkt.clouddn.com/deconvnet2.png)

这些图层显示了更多更高级别的功能，如探测狗脸或鲜花。 有一点需要注意的是，在第一个conv层之后，我们通常有一个对图像进行下采样的池（例如，将一个`32x32x3`的数据体量volumes变成一个`16x16x3`的体量）。这样做的效果是，在第二层中可以探测到在原始图像中更广泛的范围。 有关deconvnet或一般论文的更多信息，请查看Zeiler的[presentation](https://www.youtube.com/watch?v=ghEmQSxT6tw)的主题。

## 重要性

ZF Net不仅在2013年的竞争中胜出，而且在CNN的运作方面也提供了很好的直观理解，并说明了更多提高性能的方法。所描述的可视化方法不仅有助于解释CNN的内部工作原理，而且还提供了一些对网络体系结构进行改进的建议。迷人的deconv可视化方法和闭塞实验使得这篇论文成为了我个人最喜欢的论文之一。

- - -

# [VGG Net](http://arxiv.org/pdf/1409.1556v6.pdf)（2014）

VGG Net，这个2014年创建的模型（不是ILSVRC 2014的获奖者）依靠它7.3％的错误率为我们展现了它简单和深入两大特性。这是牛津大学的Karen Simonyan和Andrew Zisserman创建的一个19层的CNN，严格使用了`3x3`的过滤器，填充padding为1，最大池化层maxpooling layer为`2x2`，步幅stride为2。

![](http://ope2etmx1.bkt.clouddn.com/VGGNet.png)

## 主要观点

- 仅使用`3x3`大小的过滤器，这与应用于第一层中的AlexNet `11x11`过滤器和ZF Net的`7x7`过滤器完全不同。作者的推理是，两个`3x3`的conv层的组合具有`5x5`的有效感受域。这反过来可以模拟一个更大的过滤器，同时又能保持较小的过滤器尺寸的好处。其中一个好处是减少了参数的数量。此外，有了两个conv层，我们可以使用两个ReLU层而不是一个。
- 背靠背的三个conv的层可以实现一个`7x7`尺寸的接受领域的效果。
- 随着每层输入体量的空间尺寸减小（conv和pool层输出的结果），体量的深度也会随着过滤器数量的增加而增加。
- 有趣的是，过滤器数量在每个maxpool层之后都会增加一倍。这强化了缩小空间维度的想法，但增加了深度。
- 在图像分类和定位任务上效果很好。作者使用了一种定位方法作为回归（详见本篇[论文](http://arxiv.org/pdf/1409.1556v6.pdf)的第10页）。
- 使用Caffe工具箱（Caffe Toolbox）建立模型。
- 在训练过程中使用比例抖动（scale jittering）作为一种数据增强技术。
- 在每个conv层之后使用ReLU层，并使用批梯度下降进行训练。
- 在4个Nvidia Titan Black GPU上训练**2到3周**。

## 重要性

VGG Net是我心目中最有影响力的论文之一，因为它进一步强化了**卷积神经网络必须具有深层网络才能使视觉数据的分层表示能顺利实现**的观点。

- - -

# [GoogLeNet](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)(2015)

刚才我们谈到了简单网络架构的概念。2015年，谷歌推出了Inception模块。GoogLeNet是一个22层的CNN，是2014年ILSVRC的赢家，top 5的错误率为6.7％。据我所知，这是第一个真正不再沿用在一个顺序结构中简单地堆叠conv和pooling层叠的一般方法的CNN。这篇论文的作者还强调，这个新模型对内存和电源使用有一些考虑（重要的是，我有时也会忘记：堆叠所有这些层并添加大量的过滤器会带来计算和存储成本，以及增加过拟合的可能）。

![](http://ope2etmx1.bkt.clouddn.com/GoogleNet.gif)

![](http://ope2etmx1.bkt.clouddn.com/GoogLeNet.png)

## Inception 模块

当我们第一次接触GoogLeNet的时候，我们立即会注意到，不是所有事件都是像之前的架构那样串行顺序发生的。我们的网络中有部分功能是并行进行的。

![](http://ope2etmx1.bkt.clouddn.com/GoogLeNet2.png)

这个方块被称为Inception模块，让我们仔细看看这是由什么组成的：

![](http://ope2etmx1.bkt.clouddn.com/GoogLeNet3.png)

最下面的绿色框是我们的输入，最上面的是模型的输出（把这张照片右转90度可以让你看到上一张显示整个网络的模型的图片）。基本上，在传统的ConvNet的每一层，你必须选择是否有池操作pooling operation或卷积操作conv operation（也有筛选器大小的选择）。 Inception模块允许你做的是并行执行所有这些操作。事实上，这正是作者提出的最“朴素”的想法。

![](http://ope2etmx1.bkt.clouddn.com/GoogLeNet4.png)

那么现在为什么这行不通呢？因为这会导致出现太多的输出。我们最终会得到一个非常大的输出体量的深度频道。作者解决这个问题的方法是在`3x3`和`5x5`层之前添加`1x1`的conv操作。 `1×1`卷积（或网络层中的网络）提供了降维的方法。例如，假设你的输入体量为`100x100x60`（这不一定是图像的尺寸，只是网络任何层的输入）。应用20个`1x1`卷积滤波器可以让你将体量减小到`100x100x20`。这意味着`3x3`和`5x5`的卷积将不会有很大的处理量。这可以被认为是“特征池化pooling of features”，因为我们正在减小体量的深度，类似于我们如何使用普通的maxpooling层来减小高度和宽度的尺寸。另外值得注意的是这些`1x1`的conv层后面跟着的是不能被伤害到的ReLU units（参见Aaditya Prakash关于1x1卷积效果的[更多信息](http://iamaaditya.github.io/2016/03/one-by-one-convolution/)）。看看这个[视频](https://www.youtube.com/watch?v=VxhSouuSZDY)最后的过滤器连接的可视化。

你可能会问自己：“这个架构如何给我们提供帮助？”好，如果你有一个由网络层组成的网络，一个中等大小的卷积滤波器，一个大尺寸的卷积过滤器和一个池化操作pooling operation。网络中的网络能够提取非常详细的关于体量细节的信息，而`5x5`滤波器能够覆盖输入的较大接受区域，从而能够提取其信息。你也有一个池化操作，用来减少空间大小和预防过拟合。最重要的是，每个conv层之后都有ReLU，这有助于提高网络的非线性。基本上，网络能够执行这些不同操作的功能，同时仍然保持计算上的一致性。本文也给出了更多的高层次推理，涉及稀疏性和密集联系等主题（请阅读[论文](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)的第3和第4部分，但对我来说还不完全清楚，但如果有人有任何见解，我很乐意在评论中交流！）。

## 主要观点

- 在整个架构中使用了9个Inception模块，共有100多层。确实非常深。
- 没有使用全连接层，而使用平均池average pool来代替，从`7x7x1024`体量到`1x1x1024`体量。这节省了大量的参数。
- 使用比AlexNet少12倍的参数。
- 在测试过程中，创建多个相同的图像，并将其输入网络，并在最终结果上生成平均softmax概率。
- 使用R-CNN中的概念（我们将在后面讨论）作为检测模型。
- Inception模块有更新的版本（版本6和7）。
- 在若干个高端GPU上进行一周的**训练**。

## 重要性

GoogLeNet是第一个引入了“CNN层次并不总是必须依次叠加”概念的模型之一。 在Inception模块之后，作者们表示，在层次结构上的创造性可以提高性能和计算效率。这篇论文确实为我们在未来几年可以看到的一些令人惊叹的架构奠定了基础。

那么让我们进一步看看更深层次的内容。

- - - 

# [Microsoft ResNet](https://arxiv.org/pdf/1512.03385v1.pdf)(2015)

想象一下深度的CNN架构，接下来，将层数增加一倍，再增加几个层次，但仍然不如微软亚洲研究院在2015年年底提出的ResNet架构那么深。ResNet是一个新的152层网络架构，这是一个新的记录。这是一个可以进行分类，检测和定位的令人难以置信的架构。除了层数方面的新纪录，ResNet以3.6％的惊人错误率赢得了ILSVRC 2015（根据他们的技术和专业知识，人类通常徘徊在5-10％的错误率。可以查阅Andrej Karpathy的[文章](http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/)，里面阐述了他在ImageNet Challenge上与ConvNet竞争的经验）。


![](http://ope2etmx1.bkt.clouddn.com/ResNet.gif)

## 残差模块 Residual Block

残差块背后的想法是，当你的输入`x`通过`conv-relu-conv`系列层。这会给你一些`F(x)`。 该结果然后被添加到原始输入`x`。 我们称之为`H(x)= F(x)+x`。 在传统的CNN中，你的`H(x)`就等于`F(x)`。所以，我们不是只计算这个变换（直接从`x`到`F(x)`），我们需要计算需要往输入`x`上添加的部分——`F(x)`。 基本上，下面显示的迷你模块正在计算`delta`或着对原始输入`x`的轻微改变以获得稍微改变的结果表示（当我们考虑传统的CNN时，我们从`x`到`F(x)`，这是一个不保留关于原始x的任何信息的表示）。作者认为，“优化残差映射比优化原始的、未引用的映射更容易”。

![](http://ope2etmx1.bkt.clouddn.com/ResNet.png)

残差模块可能会有效的另一个原因在于，当逆向传递的过程中，梯度会随着图很轻易地流动，因为我们有额外添加的分割梯度的操作。

## 主要观点

- “非常深” - Yann LeCun。
- 152层。
- 有意思的是，在仅仅前两层之后，空间大小从`224×224`的输入体量被压缩到`56×56`体量。
- 作者声称，普通网络中层次的增加会导致更高的训练和测试错误（[本文](https://arxiv.org/pdf/1512.03385v1.pdf)中的图1）。
- 该研发组织尝试了一个1202层的网络，但测试的准确性较低，大概是由于过配合导致。
- 在8GPU机器上训练*2到3周*。

## 重要性

3.6％的错误率，这本身就有足够的说服力。ResNet模型是我们目前拥有的最好的CNN架构，是残留学习理念的一个伟大创新。由于自2012年以来每年的错误率都在下降，我对今年错误率是否会继续下降表示怀疑。我相信我们已经到了即便继续堆叠更多层但结果不会导致大幅提升的地步。但是，相信接下去肯定会继续诞生像我们在过去两年看到的那样有创意的新架构。

 [ResNets inside of ResNets](http://arxiv.org/pdf/1608.02908.pdf)

- - -

# 基于域的CNN：Region Based CNNs ([R-CNN](https://arxiv.org/pdf/1311.2524v5.pdf) - 2013, [Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf) - 2015, [Faster R-CNN](http://arxiv.org/pdf/1506.01497v3.pdf) - 2015)



![](http://ope2etmx1.bkt.clouddn.com/rcnn.png)


![](http://ope2etmx1.bkt.clouddn.com/FastRCNN.png)


![](http://ope2etmx1.bkt.clouddn.com/FasterRCNN.png)


![](http://ope2etmx1.bkt.clouddn.com/Adversarial.png)


![](http://ope2etmx1.bkt.clouddn.com/Caption.png)


![](http://ope2etmx1.bkt.clouddn.com/GeneratingImageDescriptions.png)


![](http://ope2etmx1.bkt.clouddn.com/SpatialTransformer.png)


![](http://ope2etmx1.bkt.clouddn.com/SpatialTransformer2.png)
