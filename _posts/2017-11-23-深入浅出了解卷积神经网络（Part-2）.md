---

layout: post
categories: [Deep Learning]
tags: [Machine Learning, Deep Learning,  Neural Network]

---

特别鸣谢：[A Beginner's Guide To Understanding Convolutional Neural Networks Part 2](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)

![](http://ope2etmx1.bkt.clouddn.com/Cover2nd.png)

# 简介

[Part1](http://kakack.github.io/2017/11/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E4%BA%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Part-1/)

在这篇文章中，我们将进一步介绍卷积神经网络（ConvNets）的更多细节。 *声明*：现在我明白了其中的一些话题是相当复杂的，可以用通篇文章来解释这些话题。 为了保持行文的简洁和全面性，我将提供有关这些主题的更详细的研究论文的链接。

- - -

# Stride和Padding（步幅和填充）

好的，让我们回头看看之前的conv层。我们知道有过滤器，接受的领域，卷积处理，那么现在还有两个主要参数可以修改每个层的行为。在我们选择过滤器大小之后，我们还必须选择步幅（Stride）和填充（Padding）。

步幅控制过滤器如何在输入体量周围进行卷积。在Part1的例子中，滤波器在输入体量周围一次转换一个单位。 步幅指的就是过滤器每次移动的数量。 在这种情况下，步幅隐式设置为1。步幅通常设置为使得输出体量是整数而不是分数。我们来看一个例子，假设有一个`7x7`的输入体量和一个3x3的滤波器（为了简单起见，不考虑第三维），步幅为1。这就是我们习惯的情况。

![](http://ope2etmx1.bkt.clouddn.com/Stride1.png)

和原先的一样，当步幅增长为2的时候，可以看看输出体量变化如何。

![](http://ope2etmx1.bkt.clouddn.com/Stride2.png)

所以，正如你所看到的那样，接受域现在每次移动2个单位，输出体量也会随之缩小。请注意，当我们试图将步幅设置为3，那么我们会遇到间距问题，所以必须确保接受域能适合当前的输入体量。通常情况下，如果希望接受域重叠较少，并且获得更小的空间维度的话，那么程序员们会适当增大步幅。

接下去，让我们来看看填充（Padding）。在开始之前，让我们来思考一个场景。当你将三个`5x5x3`的过滤器应用到一个`32x32x3`输入体量时会发生什么情况？输出体量将会是`28x28x3`。请注意，空间维度减小了。当我们继续应用转换层时，体量的大小会比我们想要的要减小地更快。在我们的网络的早期的层中，我们希望保留尽可能多的有关原始输入体量的信息，以便我们可以提取这些低级特征。假设我们想要应用相同的conv层，但是我们希望输出体量依然保持为`32x32x3`，为此，我们可以将大小为2的零填充应用于该层。将输入体量的边界填充为零。如果我们考虑一个大小为2的零填充，那么这将让我们获得一个新的`36x36x3`的输入体量。

![](http://ope2etmx1.bkt.clouddn.com/Pad.png)

如果你的步长为1，并且要设定零填充：

![](http://ope2etmx1.bkt.clouddn.com/ZeroPad.png)

其中`K`是指过滤器的尺寸，然后输入和输出体量将会一直都是相同的空间维度。

对于任何给定的卷积层，计算输出尺寸公式如下：

![](http://ope2etmx1.bkt.clouddn.com/Output.png)

其中`O`是输出的高度/长度， `W`是输入的高度/长度，`K`是过滤器尺寸，`P`是填充大小，`S`是步长。

- - -

# 选择超参数


那么我们如何知道要使用多少个图层，多少个卷积层，过滤器尺寸多大，或者步长和填充的值是多少。 这些问题并不是微不足道的，而且也没有研究人员能给出一个设定标准值。这是因为网络性能的好坏很大程度上取决于您拥有的数据类型。数据可能因大小，图像的复杂程度，图像处理任务的类型等而不同。当查看数据集时，考虑如何选择超参数的一种方法是找到正确的组合，以适当的比例创建图像的抽象。

— - -

# ReLU (Rectified Linear Units，纠正线性单元) 层

在每个conv层之后，按照惯例，紧随其后的是应用一个非线性层（或激活层）。该层的目的是将非线性特性引入刚刚在conv层期间计算线性运算后得到的的系统中（正好是元素乘法和summations）。过去，`tanh`和`sigmoid`等非线性函数被使用十分广泛，但是研究人员发现**`ReLU`**层的效果要好得多，因为网络能够训练很快（由于计算效率）而没有产生显着的准确性差异。这也有助于缓解消除梯度问题，这也就是下层网络训练非常缓慢的原因，因为梯度在各层之间呈指数级下降（解释这可能超出了本文的范围，但在[这里](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)和[这里](https://www.quora.com/What-is-the-vanishing-gradient-problem)看到为了好的描述）。 ReLU层将函数**`f(x)= max(0，x)`**应用于输入体量中的所有值。从基本的角度来说，这个层只是将所有的小于零的激活值变为0。这个层增加了模型和整个网络的非线性特性，而不影响conv层的感知域。

[深度学习之父Geoffrey Hinton的论文](http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)

- - -

# 池化层（Pooling Layers）

在一些ReLU层之后，程序员可以选择应用一个**池化层（Pooling Layer）**。它也被称为下采样层（downsampling layer）。在这个类别中，还有几个图层选项，其中`maxpooling`是最流行的。 这基本上需要有一个过滤器（通常大小为`2x2`）和一个相同长度的步幅，然后将其应用于输入体量上，并输出过滤器卷积的每个子区域中的最大值，详情如下图。

![](http://ope2etmx1.bkt.clouddn.com/MaxPool.png)

其他用于池化层的选择是**平均池化（Average Pooling）**和**L2范式池化（L2-norm pooling）**。 这一层背后的直观推理是，一旦我们知道原始输入体l量中有一个特定的特征（将有一个高激活值），其确切位置就不如它相对其它特征的相对位置那么重要。你可以想象，这个层大大减少了输入体积的空间维度（长度和宽度的变化，但不是深度）。这有两个主要目的：

- 首先是参数或权重的数量减少了75％，从而节约了计算成本。
- 二是控制**`过度拟合（Overfitting）`**。 这个术语是指当一个模型能够很好地适应训练样例的同时，却不能很好地推广到验证和测试集。过度拟合的一个症状是在训练集上有100％或99％准确率的模型，但测试数据上只有50％甚至更低的表现。

- - -

# 缺失层（Dropout Layer）

现在，缺失层（Dropout layer）在神经网络中具有非常特殊的功能，在上一节中，我们讨论了过度拟合的问题，在训练之后，网络的权重会根据给定的训练样例进行调整，但是网络在新的例子上却会表现不佳。Dropout的想法本质上很简单，该层通过将值设置成零的方式，来随机“退出”该层中的一组激活。那么这样一个简单的，看似不必要的又违反直觉的过程有什么好处呢？从某种意义上说，这就迫使网络看上去变得十分冗余。但是我的意思是，即使中间的一些激活被剔除，网络依然应该能够为特定的例子提供正确的分类或输出。它确保网络对现有的训练数据不至于太“适合”，从而有助于缓解过拟合问题。一个重要的注意事项是这个层只能在训练期间使用，而不是在测试期间使用。

详见[Geoffrey Hinton的paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

- - -

# 网络层中的网络

一个**网络层中的网络（Network in Network layer）**是指使用`1×1`大小的过滤器的conv层。乍看之下，你可能会想知道为什么这种类型的层会对我们的神经网络有所帮助，因为接受的字段通常比它们映射的空间大。但是，我们必须记住，这些1x1卷积跨越了一定的深度，所以我们可以把它看作是一个`1x1xN`卷积，其中N是层中应用的过滤器的数量。实际上，该层正在执行一个`N-D元素方式(N-D element-wise)`的乘法，其中N是进入该层的输入体量的深度。

详见[Min Lin的paper](https://arxiv.org/pdf/1312.4400v3.pdf)

- - -

# 分类、定位、检测和分割（Classification, Localization, Detection, Segmentation）

在Part1中使用的示例中，我们研究了**图像分类（Image Classification）**的任务，这是一个从获取输入图像并从一组类别中输出类别编号的过程。然而，当我们完成像**对象定位（Object localization）**这样的任务时，我们的工作不仅仅是生成一个类标签，还有需要生成一个描述对象在图片中位置的边界框。

![](http://ope2etmx1.bkt.clouddn.com/Localization.png)

我们也有**对象检测（object detection）**的任务，其中需要对图像中的所有对象进行本定位。因此，你将需要有多个边界框和多个类标签。

最后，我们还有**对象分割（ object segmentation）**，其中任务是输出类标签以及标记出输入图像中每个对象的轮廓。

![](http://ope2etmx1.bkt.clouddn.com/Detection.png)

更多细节将在Part3中提及。

- Detection/ Localization: [RCNN](https://arxiv.org/pdf/1311.2524v5.pdf), [Fast RCNN](https://arxiv.org/pdf/1504.08083.pdf), [Faster RCNN](http://arxiv.org/pdf/1506.01497v3.pdf), [MultiBox](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf), [Bayesian Optimization](http://web.eecs.umich.edu/~honglak/cvpr15-cnn-detection.pdf), [Multi-region](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Gidaris_Object_Detection_via_ICCV_2015_paper.pdf), [RCNN Minus R](http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/lenc15rcnn.pdf), [Image Windows](http://calvin.inf.ed.ac.uk/wp-content/uploads/Publications/alexe12pami.pdf)
- Segmentation: [Semantic Seg](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf), [Unconstrained Video](http://calvin.inf.ed.ac.uk/wp-content/uploads/Publications/papazoglouICCV2013-camera-ready.pdf), [Shape Guided](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/Borenstein06.pdf), [Object Regions](http://crcv.ucf.edu/papers/cvpr2013/VideoObjectSegmentation.pdf), [Shape Sharing](http://www.cs.utexas.edu/~grauman/papers/shape-sharing-ECCV2012.pdf)

当然，还有很多其他内容。

- - -

# 转换学习（Transfer Learning）

现在，深度学习社区一个常见的误解是，如果没有Google-esque级别的数据量，你不可能指望能创建有效的深度学习模型。虽然数据确实是创建网络的关键部分，但**转移学习（Transfer Learning）**的想法有助于减少数据需求。转移学习是指一个采用预先训练好的模型（由其他人对大型数据集进行训练的网络的权重和参数）以及用你自己的数据集“微调”模型的过程。这个想法指的是，这个预先训练的模型将作为一个特征提取器，你将删除网络的最后一层，并将其替换为自己的分类器（具体取决于你的实际问题）。然后冻结所有其他层的权重并正常训练网络（冻结层意味着在梯度下降/优化期间不改变权重）。

让我们来看看为什么这个方法是可行的。假设我们正在讨论的预训练模型是在ImageNet上进行训练的（对于那些不熟悉的人，ImageNet是一个包含超过1,000个类的1400万图像的数据集）。当我们考虑网络的较低层时，我们知道它们将会检测到边缘和曲线等特征。现在，除非您有一个非常独特的问题或者数据集，否则您的网络都将会需要去检测曲线和边缘。我们可以使用预先训练好的模型的权重（并冻结它们），并把重点放在更重要的层面上（比较高的层面）进行训练，而不是通过对权重的随机初始化来训练整个网络。如果你的数据集与ImageNet的数据集完全不同，那么你需要训练更多的层并冻结数量较少的一些低级层。

- [Paper](https://arxiv.org/pdf/1411.1792v1.pdf) by Yoshua Bengio (another deep learning pioneer).
- [Paper](http://arxiv.org/pdf/1403.6382.pdf) by Ali Sharif Razavian.
- [Paper](https://arxiv.org/pdf/1310.1531.pdf) by Jeff Donahue.
- [Paper](https://arxiv.org/pdf/1705.07706.pdf) and [subsequent paper](https://arxiv.org/pdf/1707.09872.pdf) by Dario Garcia-Gasulla.

- - -

# 数据增强技术（Data Augmentation Techniques）

到目前为止，我们都可能任然对ConvNets数据的重要性感到麻木，所以让我们来谈谈如何让现有的数据集更大，这只需要一些简单的转换即可。就像我们之前提到的那样，当一台计算机将图像作为输入时，它将采用一组像素值。假设整个图像左移1个像素。对你我来说，这种变化是不可察觉的。然而，对于计算机来说，这种转变可能相当重要，因为图像的分类或标签没有改变，但是数组却改变了。通过微调表示数组的方法改变训练数据，但仍保持标签相同的方式称为**数据增强技术Data Augmentation Techniques）**。这是一种人为地扩展数据集的方法。人们常使用的一些流行的增强方法是灰度，水平翻转，垂直翻转，随机修剪，颜色抖动，翻转，旋转等等。通过将这些转换应用到当前的训练数据中，你可以轻松地将训练示例的数量增加一到三倍。

[Link to Part3](http://kakack.github.io/2017/11/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E4%BA%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Part-3/)
