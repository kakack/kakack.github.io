---

layout: post
tags: [LLM, NLP, Inference]
title: Efficient Inference for LLMs
date: 2024-05-17
author: Kaka Chen
comments: true
toc: true
pinned: false

---

# 1 - Intro

# 2 - Algorithm-Level Inference Acceleration

## 2.1 - Speculative Decoding

## 2.2 - KV-Cache Optimization

# 3 - System-Level Inference Acceleration

# 4 - Conclusion