---

layout: post
categories: [Algorithm]
tags: [Deep Learning, Model Compression]

---

# Abstract

随着Deep NN的广泛运用，我们发现如果每次都使用体量巨大的NN模型，会对计算资源要求非常高，因此一些关于模型压缩和加速的手段被提出来，尤其是在将模型应用到移动端的尝试，让模型应用有了如下一些优势：

- 减轻服务器端计算压力，利用云端一体化实现负载均衡；
- 实时性好，响应速度快；
- 稳定性高，可靠性好；
- 安全性高，用户隐私保护好。

其中模型的压缩和加速是两个不同的方面，它们之间有可能互相有正向影响，也有可能并无相关性，其中压缩的重点在于减少网络参数量，而加速则关注降低计算复杂性，提升并行计算能力等。总体可分为三个层次：

- 算法层压缩加速：结构优化（矩阵分解、分组卷积、小卷积核等）、量化与定点化、模型剪枝、模型蒸馏；
- 框架层加速：编译优化、缓存优化、稀疏存储和计算、NEON指令应用、算子优化等；
- 硬件层加速：AI硬件芯片层加速，如GPU、FPGA、ASIC等多种方案。

简单总结一下常被提及的四种方法：

|Theme Name|Description|Application|More Details|Drawback|
|:----:|:----:|:----:|----|----|
|参数剪枝和共享（Parameter Pruning and Sharing）|减少对性能不敏感的冗余参数|卷积层和全连接层|对不同设置有鲁棒性，能够实现好的性能，能支持脚本（Scratch）和预训练模型的训练|1）量化和二进制：二进制网络的精度损失比较大；2）剪枝和共享：更难以收敛，要微调参数，不够灵活；3）设计结构矩阵：结构矩阵会伤害到模型性能，且结构矩阵不好找。|
|低秩分解（Low-rank factorization）|使用矩阵/张量分解去估计有信息量的参数|卷积层和全连接层|标准的管道（pipeline），容易执行，能够支持脚本和预训练模型。|1）分解的操作不易执行，其计算消耗是昂贵的；2）当前的算法时逐层进行的，因此不能全局压缩；3）分解后，模型的收敛难度增加。|
|转移/紧致卷积滤波器（Transferred/compact convolutional filters）|设计特殊结构的卷积滤波器去保存参数|卷积层|算法依赖于应用，通常实现好的性能，只支持脚本中训练|1）只支持宽的神经网络而不支持深度的神经网络；2）转移的假设性太强导致某情况的结构不太稳定。|
|知识蒸馏（Knowledge distillation）|通过大模型中蒸馏知识训练一个紧致的神经网络|卷积层和全连接层|模型的性能对应用是敏感的并且网络结构只支持从脚本中训练|1）目前知识蒸馏只支持softmax损失的分类模型；2）与其他方法相比，模型假设太严格了。|

# Parameter Pruning and Sharing

# Low-Rank Factorization and Sparsity

# Transfered/Compact Convolutional Filters

# Knowledge Distillation
